dataset_reader:
  type: goodnews
  tokenizer:
    type: word
    word_splitter: just_spaces
  token_indexers:
    roberta:
      type: roberta
      model_name: roberta-base
      namespace: bpe
      padding_on_right: true
      padding_value: 1
      max_len: 512
  image_dir: data/goodnews/images_processed
  max_paragraphs: 32
  lazy: true
train_data_path: train
validation_data_path: val
test_data_path: test
vocabulary:
  type: roberta
  directory_path: ./expt/vocabulary
model:
  type: transformer
  decoder:
    type: dynamic_conv_decoder
    embedder:
      type: sum
      token_embedders:
        adaptive:
          type: adaptive
          vocab_size: 50265
          namespace: bpe
          initial_dim: 1024
          output_dim: 1024
          factor: 1
          cutoff: [5000, 20000]
          padding_idx: 0
          scale_embeds: true
        position:
          type: sinusoidal_positional
          init_size: 512
          embedding_dim: 1024
          padding_idx: 1
          left_pad: false
      embedder_to_indexer_map:
        adaptive: ["roberta"]
        position: ["roberta"]
      allow_unmatched_keys: true
    max_target_positions: 512
    dropout: 0.1
    share_decoder_input_output_embed: true
    decoder_output_dim: 1024
    decoder_conv_dim: 1024
    decoder_glu: true
    decoder_conv_type: dynamic
    weight_softmax: true
    decoder_attention_heads: 16
    weight_dropout: 0.1
    relu_dropout: 0.0
    input_dropout: 0.1
    decoder_normalize_before: false
    attention_dropout: 0.1
    decoder_ffn_embed_dim: 4096
    decoder_kernel_size_list: [3, 7, 15, 31]
    adaptive_softmax_cutoff: [5000, 20000]
    adaptive_softmax_factor: 1
    tie_adaptive_weights: true
    adaptive_softmax_dropout: 0
    tie_adaptive_proj: false
    decoder_layers: 4
    context_embed_sizes: [1024, 1024]
    final_norm: false
    padding_idx: 0
    namespace: bpe
    vocab_size: 50265
  criterion:
    type: adaptive_loss
    padding_idx: 1
  use_context: true
  evaluate_mode: false
  sampling_topk: 1
  vocab_size: 50265
  hidden_size: 1024
  attention_dim: 1024
  namespace: bpe
  index: roberta
  padding_value: 1
  initializer:
    - - ^(attention|article_attention|init|f).*weight
      - type: xavier_uniform
    - - ^(attention|article_attention|init|f).*bias
      - type: zero
    - - ^rnn_cell.*weight
      - type: xavier_uniform
    - - ^rnn_cell.*bias
      - type: zero
iterator:
  type: bucket
  sorting_keys:
    - - context
      - num_fields
    - - context
      - list_num_tokens
    - - caption
      - num_tokens
  batch_size: 16
  max_instances_in_memory: 8192
  biggest_batch_first: false
  instances_per_epoch: 65536
  maximum_samples_per_batch: ["total_num_tokens", 20480]
validation_iterator:
  type: bucket
  sorting_keys:
    - - context
      - num_fields
    - - context
      - list_num_tokens
    - - caption
      - num_tokens
  batch_size: 16
  max_instances_in_memory: 8192
  maximum_samples_per_batch: ["total_num_tokens", 20480]
  biggest_batch_first: false
trainer:
  type: callback_apex
  apex_opt_level: O2
  keep_batchnorm_fp32: None
  optimizer:
    type: adam_w
    lr: 0.0001
    betas: [0.9, 0.999]
    eps: 0.000001
    weight_decay: 0.00001
    correct_bias: true
    parameter_groups:
      - - - ^attention
        - {}
      - - - ^init
        - {}
      - - - ^f
        - {}
      - - - ^rnn_cell
        - {}
  no_grad:
    - ^resnet
    - ^roberta
  num_epochs: 40
  shuffle: true
  cuda_device: 0
  callbacks:
    - type: checkpoint
    - type: track_metrics
      patience: 500
    - type: validate
    - type: log_to_tensorboard
      summary_interval: 500
      should_log_parameter_statistics: false
      log_batch_size_period: 5000
    - type: update_learning_rate
      learning_rate_scheduler:
        type: slanted_triangular
        num_epochs: 40
        num_steps_per_epoch: 4196
        cut_frac: 0.1
        ratio: 32
        gradual_unfreezing: false
        discriminative_fine_tuning: false
        decay_factor: 0.38
