% !TEX root = main.tex

\section{Related Works}

A large number of methods exist for generating generic image captions that
describe objects and relationships using only the image as input. Many of these
captioning systems use some combination of a Convolutional Neural Network
encoder and an RNN with a closed
vocabulary as a decoder~\cite{Karpathy2015DeepVA, Donahue2015LongTR,
Vinyals2015ShowAT}.
Attention over image patches was introduced in ``Show, Attend and Tell"
\cite{Xu2015ShowAA}, in which the attention weights are obtained by feeding the
image embeddings and the previous hidden state of the RNN through a multilayer
perception. Many extensions to these models have been proposed such as giving
the model the option to not
attend to any image region~\cite{Lu2017KnowingWT}, using reinforcement learning
to directly optimise for the CIDEr metric~\cite{Rennie2017SelfCriticalST,
Gao2019DeliberateAN}, and using a bottom-up approach to propose a region to
attend to \cite{Anderson2017BottomUpAT}. All of these systems generate
restricted vocabulary generic captions without considering context external to
the image.

A related task which does consider image context is news image captioning,
where the image caption is generated using the article text as context.
One key challenge of news image captioning is generating rare entity names, for
example the names of people who do not make many media appearances. Early
non-neural approaches include
extractive methods that use n-gram models to combine existing phrases
\cite{Feng2013AutomaticCG} or simply retrieving the most representative
sentence \cite{Tariq2017ACE} in the article. The neural network approach taken
by Ramisa \etal
\cite{Ramisa2016BreakingNewsAA} was able to generate entirely new text with an
LSTM decoder that took as input a word2vec representation of the
article concatenated with a CNN representation of the image. Even so this
approach was unable produce names that were not seen during training.

To overcome the limitation of a fixed-size vocabulary caption templates can be
used. This involves first generating a
template sentence with placeholders for named entities, e.g. ``PERSON speaks at
BUILDING in
DATE.''. This template can be generated using an LSTM or other sequence
generation model~\cite{Biten2019GoodNews}. Afterwards, a selection algorithm
picks the best candidate
for each placeholder. For example, Lu \etal \cite{Lu2018EntityAI} built a
knowledge graph
for each combination of entities and select the most likely combination.
Meanwhile, Biten \etal~\cite{Biten2019GoodNews} filled the template by
extracting entities from the sentence in the article which had the highest
cosine similarity to
the template. One key difference between our proposed model and that of
previous approaches\cite{Biten2019GoodNews,Lu2018EntityAI} is that our model can
generate a captions with named entities directly -- without using an
intermediate template.

%, whose GoodNews dataset we will
%benchmark against,

\begin{figure*}[t]
    \begin{center}
    \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
    \end{center}
       \caption{Overall architecture of the model.}
    \label{fig:short}
 \end{figure*}

One tool that has had seen recent successes in many natural language processing
tasks is the transformer neural network.
%\sout{processing, which consist of layers of
%	self-attention and feed-forward connections stacked on top of each other
%	\cite{Vaswani2017AttentionIA}. Transformers have}
Transformers have been show to consistently
outperforming Recurrent Neural Network architectures in language modeling
\cite{Radford2019LanguageMA},
story generation \cite{Fan2018HierarchicalNS}, summarization
\cite{Subramanian2019OnEA}, and machine translation \cite{Bojar2018Findings}.
%\sout{There are theoretical justifications that transformers could be universal
%	approximations of sequence-to-sequence functions
%\cite{Anonymous2020AreTU}.}
Furthermore, transformer based models such as BERT \cite{Devlin2019BERT}, XLM
\cite{Lample2019CrosslingualLM}, XLNet \cite{Yang2019XLNetGA}, RoBERTa
\cite{Liu2019RoBERTaAR}, and ALBERT \cite{Lan2019ALBERT} have been shown to
produce high level text representations suitable for transfer learning.
Furthermore, using byte-pair
encoding (BPE) \cite{Sennrich2015NeuralMT} to represent uncommon words as a
sequence of subword units can enable the transformer function in an open
vocabulary setting.

Transformers have been shown
to yield competitive results in generating generic MS COCO captions
\cite{Zhu2018CaptioningTW, Li2019Boosted}. Zhao \etal
\cite{Zhao2019InformativeIC} have gone further and trained transformers to
produce some named entities in the Conceptual Captions dataset
\cite{Sharma2018ConceptualCA}. However, this dataset provides no additional
context to the image, making it a different problem to news image captioning.
Moreover, the authors used web-entity labels,
extracted using Google Cloud Vision API, as inputs to the model. In our work,
we do not explicitly give the model a list of
entities to appear in the caption, instead our model automatically identifies
relevant entities from the provided news article.

BPE offers an elegant solution to handling an open vocabulary. To date the only
image captioning work that uses BPE is \cite{Sharma2018ConceptualCA}, but they
did not use it for rare named entities as these were removed from
the captions during pre-processing. In contrast we explicitly examine the use
of BPE for generating rare names and evaluate it in comparison to
template-based methods.

%We fill this gap and in particular examine how much
%better BPE can generate rare names compared to template-based methods.

In addition to attending to image patches, some captioning models also attend
to object regions \cite{Wang2019Hierarchical} and visual concepts
\cite{You2016ImageCW,Li2019Boosted,Wang2019Hierarchical}, both of which are
derived from the image itself. When attending to more than one modality, there
are various strategies on how to combine embeddings such as addition,
concatenation, and multivariate residual modules (MRMs)
\cite{Kim2016MultimodalRL}. In our model we use the vector concatenation
strategy and leave investigation of the more complex strategies, such as MMRs,
to future work as they typically only yield minor performance
improvements~\cite{Wang2019Hierarchical}.
