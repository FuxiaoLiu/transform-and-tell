% !TEX root = main.tex

% \begin{table}[t]
% 	\caption {Summary of datasets}
% 	\label{tab:datasets}
% 	\centering
% 	\begin{tabular}{llll}
% 		\toprule
% 		  & GoodNews  & GoodNews+ &   NYTimes800k \\
% 		\midrule
%       No. of articles & 241 808 & 241 808 & 445 828 \\
%       No. of images   & 462 642 & 440 112 & 794 085 \\
%       Article length & 451 & 963 & 974 \\
%       Caption length & 18 & 18 & 18 \\
%       Start month & Jan 10 & Jan 10 & Mar 05\\
%       End month & Jul 18 & Jul 18 & Sep 19 \\
% 		\bottomrule
% 	\end{tabular}
% \end{table}


\begin{table}[t]
	\caption {Summary of news captioning datasets}
	\label{tab:datasets}
	\centering
	\begin{tabularx}{\linewidth}{lXX}
		\toprule
		  & GoodNews  &   NYTimes800k \\
		\midrule
      Number of articles & 257 033 & 445 819 \\
      Number of images   & 462 642 & 794 044 \\
      Average article length & 451 & 974 \\
      Average caption length & 18 & 18 \\
      Collection start month & Jan 10 & Mar 05\\
      Collection end month & Mar 18 & Sep 19 \\
      \midrule
      \% of words that are \\
      \quad -- nouns & 16\% & 16\% \\
      \quad -- pronouns & 1\% & 1\% \\
      \quad -- proper nouns & 23\% & 22\% \\
      \quad -- verbs & 9\% & 9\%  \\
      \quad -- adjectives & 4\% & 4\% \\
      \quad -- named entities & 27\% & 26\% \\
      \quad -- personal names & 9\% & 9\% \\
      \midrule
      \% of captions with \\
      \quad -- named entities & 97\% & 96\% \\
      \quad -- personal names & 68\% & 68\% \\
		\bottomrule
	\end{tabularx}
\end{table}

\section{Datasets}

\subsection{GoodNews}

To compare to existing approaches we use the GoodNews dataset, which until now
was largest
dataset for news image captioning~\cite{Biten2019GoodNews}. Each example in the
dataset is a triplet containing an article, an image, and a caption. Since only
the article text, captions, and image URLs are publicly released
the images need to be downloaded from the original source. Out of the 466K
image
URLs provided by
\cite{Biten2019GoodNews}, we were able to download 463K images, or 99.2\% of the
original dataset -- the remaining are broken links.

We use this 99.2\% sample of the GoodNews dataset and the
train-validation-test split provided by~\cite{Biten2019GoodNews}. There are
421K training, 18K validation, and 23K test captions. Note that
this split was performed at the level of captions, so it is possible for a
training and test
caption to share the same article text (since articles have multiple images).





\begin{figure}[t]
   \begin{center}
   \includegraphics[width=0.99\linewidth]{figures/figure_2_entities.pdf}
   \end{center}
      \caption{Entity distribution in NYTimes800k training captions. The four
               most common entity types are personal names, geopolitical
               entities, organizations, and dates.}
   \label{fig:entities}
\end{figure}

\begin{figure}[t]
   \begin{center}
   \includegraphics[width=0.99\linewidth]{figures/figure_3_faces.pdf}
   \end{center}
      \caption{Co-occurrence of faces and personal names in NYTimes800k
               training data. The blue bars count how many images containing a
               certain number of faces. The orange bars count how many captions
               containing a certain number of personal names.}
   \label{fig:faces}
\end{figure}


\subsection{NYTimes800k}
\label{ssec:nytimes800k}

We constructed the NYTimes800k which is an 80\% larger and more complete
dataset
of New York Times articles, images, and captions. The construction of this
dataset was motivated by the desire to clean up data quality issues in the
GoodNews dataset (as described below), collect a larger dataset, and include
fine grained context
such as the images location in the article.

We observed that many of the articles in the GoodNews dataset had
been partially extracted when the generic article extractor used
failed to recognise some of the HTML tags used
specifically by the New York Times. Importantly, the missing text often
included the first few
paragraphs
which frequently contain important information for captioning images. To
collect the full articles for the NYTimes800k dataset we
implemented a custom parser using The New York Times public
API\footnote{\href{https://developer.nytimes.com/apis}{https://developer.nytimes.com/apis}}.
After this recollection, we observe that the average article is 963 words in
comparison to GoodNews where the average length is 451 words. In addition, we
found that GoodNews contains a
small number of non-English articles, and captioned images from the
recommendation sidebar which are not related to the main article. These were
filtered out in the construction of NYTimes880k.


Table~\ref{tab:datasets} presents a comparison between GoodNews and
NYTimes800k. NYTimes800k exhibits several advantages:

\begin{itemize}
   \item By increasing the collection period to the last 14 years (March 2005
   -- September 2019), NYTimes800k contains 80\% more articles and images, thus
   becoming the largest news image captioning dataset.
   \item Using our custom parser, articles in NYTimes800k contain the full text
   with no missing paragraphs.
   \item NYTimes800k contains only English articles.
   \item We are careful to include only images that are part of the main
   article.
   \item Unlike GoodNews, we also collect information about where an image is
   located in the corresponding article. Most news articles have one image at
   the top that relates to the key topic. However 39\% of the articles have at
   least one more image somewhere in the middle of text. The image placement
   and hence the text surrounding the image is important information for
   captioning as we show in our evaluations.
\end{itemize}

Entities play an important role in the dataset, with 97\% of captions
containing at least one named entity. As shown in Figure~\ref{fig:entities},
the most popular entity type are names of people, comprising a third of all
named entities. Furthermore, 71\% of training images contain at least one face
and 68\% of training captions mention at least one persons name. Figure
\ref{fig:faces} provides a further breakdown of the co-occurrence of faces and
personal names. One important observation is that the majority of captions
contain at most four names.


\begin{table}[t]
	\caption {NYTimes800k training, validation, and test splits}
	\label{tab:splits}
	\centering
	\begin{tabularx}{\linewidth}{lXXX}
		\toprule
		  & Training  &   Validation & Test \\
		\midrule
      Number of articles & 434 272 & 3 052 & 8 495 \\
      Number of images  & 764 049 & 7 852 & 22 143 \\
      Start month & Mar 15 & May 19 & Jun 19 \\
      End month & Apr 19 & May 19 & Aug 19 \\
		\bottomrule
	\end{tabularx}
\end{table}

\begin{figure*}[t]
   \begin{center}
   \fbox{\rule{0pt}{3in} \rule{.9\linewidth}{0pt}}
   \end{center}
      \caption{Example captions with success and failure cases.}
   \label{fig:short}
\end{figure*}


We split the training, validation, and test sets according to time, as shown in
Table~\ref{tab:splits}. For example, the test set consists of all captions and
articles in the final three months of the collection period, from June to
August 2019. This has two advantages over the random split used in GoodNews.
Firstly, it prevents captions in the training and test sets from sharing the
same context article, which allows us to evaluate how well the model can
generalize to new articles. Secondly, due of the shift in the coverage of news
over time, there
will be events and people in the test data that have never been covered by the
news
before. In particular, out of the 100K proper nouns in the test captions, 4\%
never appear in any training captions. Half of these also never appear in any
training article. Thus splitting by time allows us study how well the model can
generate rare names.
