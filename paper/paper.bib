@article{Anderson2017BottomUpAT,
    author       = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
    date         = {2017},
    journal = {ArXiv},
    title        = {Bottom-Up and Top-Down Attention for Image Captioning and VQA},
    volume       = {abs/1707.07998},
}

@inproceedings{Biten2019GoodNews,
    author    = {Biten, Ali Furkan and Gomez, Lluis and Rusinol, Mar{ç}al and Karatzas, Dimosthenis},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    date      = {2019},
    pages     = {12466--12475},
    title     = {Good News, Everyone! Context driven entity-aware captioning for news images},
}

@inproceedings{Bojar2018Findings,
    author    = {Bojar, Ond{ř}ej and Federmann, Christian and Fishel, Mark and Graham, Yvette and Haddow, Barry and Koehn, Philipp and Monz, Christof},
    location  = {Belgium, Brussels},
    publisher = {Association for Computational Linguistics},
    url       = {https://www.aclweb.org/anthology/W18-6401},
    booktitle = {Proceedings of the Third Conference on Machine Translation: Shared Task Papers},
    date      = {2018-10},
    doi       = {10.18653/v1/W18-6401},
    pages     = {272--303},
    title     = {Findings of the 2018 Conference on Machine Translation ({WMT}18)},
}

@article{Cao2017VGGFace2AD,
    author       = {Cao, Qiong and Shen, Li and Xie, Weidi and Parkhi, Omkar M. and Zisserman, Andrew},
    date         = {2017},
    journal = {2018 13th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2018)},
    pages        = {67--74},
    title        = {VGGFace2: A Dataset for Recognising Faces across Pose and Age},
}

@article{Chen2015MicrosoftCC,
    author       = {Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{á}r, Piotr and Zitnick, C. Lawrence},
    date         = {2015},
    journal = {ArXiv},
    title        = {Microsoft COCO Captions: Data Collection and Evaluation Server},
    volume       = {abs/1504.00325},
}

@inproceedings{Devlin2019BERT,
    abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
    author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    location  = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    date      = {2019-06},
    doi       = {10.18653/v1/N19-1423},
    pages     = {4171--4186},
    title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
}

@inproceedings{Fan2018HierarchicalNS,
    author    = {Fan, Angela and Lewis, Mike and Dauphin, Yann},
    booktitle = {ACL},
    date      = {2018},
    title     = {Hierarchical Neural Story Generation},
}

@article{Fang2014FromCT,
    author       = {Fang, Hao and Gupta, Saurabh and Iandola, Forrest N. and Srivastava, Rupesh Kumar and Deng, Li and Doll{á}r, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C. and Zitnick, C. Lawrence and Zweig, Geoffrey},
    date         = {2014},
    journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages        = {1473--1482},
    title        = {From captions to visual concepts and back},
}

@article{Feng2013AutomaticCG,
    author       = {Feng, Yansong and Lapata, Mirella},
    date         = {2013},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    pages        = {797--812},
    title        = {Automatic Caption Generation for News Images},
    volume       = {35},
}

@inproceedings{Gardner2017AllenNLP,
    author = {Gardner, Matt and Grus, Joel and Neumann, Mark and Tafjord, Oyvind and Dasigi, Pradeep and Liu, Nelson F. and Peters, Matthew and Schmitz, Michael and Zettlemoyer, Luke S.},
    date   = {2017},
    eprint = {arXiv:1803.07640},
    title  = {AllenNLP: A Deep Semantic Natural Language Processing Platform},
}

@article{Grave2016EfficientSA,
    author       = {Grave, Edouard and Joulin, Armand and Ciss{é}, Moustapha and Grangier, David and J{é}gou, Herv{é}},
    date         = {2016},
    journal = {ArXiv},
    title        = {Efficient softmax approximation for GPUs},
    volume       = {abs/1609.04309},
}

@inproceedings{Karpathy2015DeepVA,
    author    = {Karpathy, Andrej and Fei-Fei, Li},
    booktitle = {CVPR},
    date      = {2015},
    title     = {Deep visual-semantic alignments for generating image descriptions},
}

@article{Lample2019CrosslingualLM,
    author       = {Lample, Guillaume and Conneau, Alexis},
    date         = {2019},
    journal = {ArXiv},
    title        = {Cross-lingual Language Model Pretraining},
    volume       = {abs/1901.07291},
}

@article{Lan2019ALBERT,
    author       = {Lan, Zhen-Zhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
    date         = {2019},
    journal = {ArXiv},
    title        = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
    volume       = {abs/1909.11942},
}

@article{Li2019Boosted,
    author       = {Li, Jiangyun and Yao, Peng and Guo, Longteng and Zhang, Weicun},
    publisher    = {Multidisciplinary Digital Publishing Institute},
    date         = {2019},
    journal = {Applied Sciences},
    number       = {16},
    pages        = {3260},
    title        = {Boosted Transformer for Image Captioning},
    volume       = {9},
}

@inproceedings{Lin2014MicrosoftCC,
    author    = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge J. and Bourdev, Lubomir D. and Girshick, Ross B. and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{á}r, Piotr and Zitnick, C. Lawrence},
    booktitle = {ECCV},
    date      = {2014},
    title     = {Microsoft COCO: Common Objects in Context},
}

@article{Liu2019RoBERTaAR,
    author       = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar S. and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke S. and Stoyanov, Veselin},
    date         = {2019},
    journal = {ArXiv},
    title        = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    volume       = {abs/1907.11692},
}

@article{Lu2016KnowingWT,
    author       = {Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
    date         = {2016},
    journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages        = {3242--3250},
    title        = {Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning},
}

@inproceedings{Mikolov2013DistributedRO,
    author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Gregory S. and Dean, Jeffrey},
    booktitle = {NIPS},
    date      = {2013},
    title     = {Distributed Representations of Words and Phrases and their Compositionality},
}

@inproceedings{Ott2019Fairseq,
    author    = {Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
    booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
    date      = {2019},
    title     = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
}

@inproceedings{Paszke2017Automatic,
    author    = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
    booktitle = {NIPS Autodiff Workshop},
    date      = {2017},
    title     = {Automatic Differentiation in {PyTorch}},
}

@inproceedings{Pennington2014Glove,
    author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
    url       = {http://www.aclweb.org/anthology/D14-1162},
    booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
    date      = {2014},
    pages     = {1532--1543},
    title     = {GloVe: Global Vectors for Word Representation},
}

@inproceedings{Radford2019LanguageMA,
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    date   = {2019},
    title  = {Language Models are Unsupervised Multitask Learners},
}

@inproceedings{Rajpurkar2016SQuAD,
    author    = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
    location  = {Austin, Texas},
    publisher = {Association for Computational Linguistics},
    url       = {https://www.aclweb.org/anthology/D16-1264},
    booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
    date      = {2016-11},
    doi       = {10.18653/v1/D16-1264},
    pages     = {2383--2392},
    title     = {{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text},
}

@inproceedings{Rajpurkar2018KnowWY,
    abstract  = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.},
    author    = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
    location  = {Melbourne, Australia},
    publisher = {Association for Computational Linguistics},
    url       = {https://www.aclweb.org/anthology/P18-2124},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    date      = {2018-07},
    doi       = {10.18653/v1/P18-2124},
    pages     = {784--789},
    title     = {Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}},
}

@article{Ramisa2016BreakingNewsAA,
    author       = {Ramisa, Arnau and Yan, Fei and Moreno-Noguer, Francesc and Mikolajczyk, Krystian},
    date         = {2016},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    pages        = {1072--1085},
    title        = {BreakingNews: Article Annotation by Image and Text Processing},
    volume       = {40},
}

@inproceedings{Ren2015FasterRCNN,
    author    = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
    booktitle = {Advances in neural information processing systems},
    date      = {2015},
    pages     = {91--99},
    title     = {Faster r-cnn: Towards real-time object detection with region proposal networks},
}

@article{Rennie2016SelfCriticalST,
    author       = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
    date         = {2016},
    journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages        = {1179--1195},
    title        = {Self-Critical Sequence Training for Image Captioning},
}

@inproceedings{Sennrich2015NeuralMT,
    author    = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
    location  = {Berlin, Germany},
    publisher = {Association for Computational Linguistics},
    url       = {https://www.aclweb.org/anthology/P16-1162},
    booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    date      = {2016-08},
    doi       = {10.18653/v1/P16-1162},
    pages     = {1715--1725},
    title     = {Neural Machine Translation of Rare Words with Subword Units},
}

@article{Schroff2015FaceNetAU,
    author       = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
    date         = {2015},
    journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages        = {815--823},
    title        = {FaceNet: A unified embedding for face recognition and clustering},
}

@article{Subramanian2019OnEA,
    author       = {Subramanian, Sandeep and Li, Raymond and Pilault, Jonathan and Pal, Christopher Joseph},
    date         = {2019},
    journal = {ArXiv},
    title        = {On Extractive and Abstractive Neural Document Summarization with Transformer Language Models},
    volume       = {abs/1909.03186},
}

@article{Tariq2017ACE,
    author       = {Tariq, Amara and Foroosh, Hassan},
    date         = {2017},
    journal = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
    pages        = {619--632},
    title        = {A Context-Driven Extractive Framework for Generating Realistic Image Descriptions.},
    volume       = {26 2},
}

@inproceedings{Vaswani2017AttentionIA,
    author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
    booktitle = {NIPS},
    date      = {2017},
    title     = {Attention Is All You Need},
}

@article{Vinyals2014ShowAT,
    author       = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
    date         = {2014},
    journal = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages        = {3156--3164},
    title        = {Show and tell: A neural image caption generator},
}

@inproceedings{Wang2019GLUE,
    author    = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
    url       = {https://openreview.net/forum?id=rJ4km2R5t7},
    booktitle = {International Conference on Learning Representations},
    date      = {2019},
    title     = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
}

@article{Wang2019SuperGLUEAS,
    author       = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
    date         = {2019},
    journal = {ArXiv},
    title        = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
    volume       = {abs/1905.00537},
}

@inproceedings{Wang2019Hierarchical,
    author    = {Wang, Weixuan and Chen, Zhihong and Hu, Haifeng},
    booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
    date      = {2019},
    pages     = {8957--8964},
    title     = {Hierarchical attention network for image captioning},
    volume    = {33},
}

@inproceedings{Wu2018PayLA,
    author    = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann and Auli, Michael},
    url       = {https://openreview.net/forum?id=SkVhlh09tX},
    booktitle = {International Conference on Learning Representations},
    date      = {2019},
    title     = {Pay Less Attention with Lightweight and Dynamic Convolutions},
}

@article{Xie2016AggregatedRT,
    author       = {Xie, Saining and Girshick, Ross B. and Doll{á}r, Piotr and Tu, Zhuowen and He, Kaiming},
    date         = {2016},
    journal = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages        = {5987--5995},
    title        = {Aggregated Residual Transformations for Deep Neural Networks},
}

@inproceedings{Xu2015ShowAA,
    author    = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron C. and Salakhutdinov, Ruslan and Zemel, Richard S. and Bengio, Yoshua},
    booktitle = {ICML},
    date      = {2015},
    title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
}

@article{Yang2019XLNetGA,
    author       = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime G. and Salakhutdinov, Ruslan and Le, Quoc V.},
    date         = {2019},
    journal = {ArXiv},
    title        = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
    volume       = {abs/1906.08237},
}

@article{Young2014FromID,
    author       = {Young, Peter and Lai, Alice and Hodosh, Micah and Hockenmaier, Julia},
    date         = {2014},
    journal = {Transactions of the Association for Computational Linguistics},
    pages        = {67--78},
    title        = {From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions},
    volume       = {2},
}

@article{Zhang2016JointFD,
    author       = {Zhang, Kaipeng and Zhang, Zhanpeng and Li, Zhifeng and Qiao, Yu},
    date         = {2016},
    journal = {IEEE Signal Processing Letters},
    pages        = {1499--1503},
    title        = {Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks},
    volume       = {23},
}

@article{Zhu2018CaptioningTW,
    author       = {Zhu, Xinxin and Li, Lixiang and Liu, Jing and Peng, Haipeng and Niu, Xinxin},
    publisher    = {Multidisciplinary Digital Publishing Institute},
    date         = {2018},
    journal = {Applied Sciences},
    number       = {5},
    pages        = {739},
    title        = {Captioning transformer with stacked attention modules},
    volume       = {8},
}

