@article{Anderson2017BottomUpAT,
    author       = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
    date         = {2017},
    journaltitle = {ArXiv},
    title        = {Bottom-Up and Top-Down Attention for Image Captioning and VQA},
    volume       = {abs/1707.07998},
}

@inproceedings{Biten2019GoodNews,
    author    = {Biten, Ali Furkan and Gomez, Lluis and Rusinol, Mar{ç}al and Karatzas, Dimosthenis},
    booktitle = {Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition},
    date      = {2019},
    pages     = {12466--12475},
    title     = {Good News, Everyone! Context driven entity-aware captioning for news images},
}

@article{Cao2017VGGFace2AD,
    author       = {Cao, Qiong and Shen, Li and Xie, Weidi and Parkhi, Omkar M. and Zisserman, Andrew},
    date         = {2017},
    journaltitle = {2018 13th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2018)},
    pages        = {67--74},
    title        = {VGGFace2: A Dataset for Recognising Faces across Pose and Age},
}

@inproceedings{Devlin2019BERT,
    abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
    author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    location  = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    date      = {2019-06},
    doi       = {10.18653/v1/N19-1423},
    pages     = {4171--4186},
    title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
}

@article{Feng2013AutomaticCG,
    author       = {Feng, Yansong and Lapata, Mirella},
    date         = {2013},
    journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    pages        = {797--812},
    title        = {Automatic Caption Generation for News Images},
    volume       = {35},
}

@inproceedings{Gardner2017AllenNLP,
    author = {Gardner, Matt and Grus, Joel and Neumann, Mark and Tafjord, Oyvind and Dasigi, Pradeep and Liu, Nelson F. and Peters, Matthew and Schmitz, Michael and Zettlemoyer, Luke S.},
    date   = {2017},
    eprint = {arXiv:1803.07640},
    title  = {AllenNLP: A Deep Semantic Natural Language Processing Platform},
}

@article{Grave2016EfficientSA,
    author       = {Grave, Edouard and Joulin, Armand and Ciss{é}, Moustapha and Grangier, David and J{é}gou, Herv{é}},
    date         = {2016},
    journaltitle = {ArXiv},
    title        = {Efficient softmax approximation for GPUs},
    volume       = {abs/1609.04309},
}

@inproceedings{Lin2014MicrosoftCC,
    author    = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge J. and Bourdev, Lubomir D. and Girshick, Ross B. and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{á}r, Piotr and Zitnick, C. Lawrence},
    booktitle = {ECCV},
    date      = {2014},
    title     = {Microsoft COCO: Common Objects in Context},
}

@article{Liu2019RoBERTaAR,
    author       = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar S. and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke S. and Stoyanov, Veselin},
    date         = {2019},
    journaltitle = {ArXiv},
    title        = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    volume       = {abs/1907.11692},
}

@article{Lu2016KnowingWT,
    author       = {Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
    date         = {2016},
    journaltitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages        = {3242--3250},
    title        = {Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning},
}

@inproceedings{Mikolov2013DistributedRO,
    author    = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Gregory S. and Dean, Jeffrey},
    booktitle = {NIPS},
    date      = {2013},
    title     = {Distributed Representations of Words and Phrases and their Compositionality},
}

@inproceedings{Ott2019Fairseq,
    author    = {Ott, Myle and Edunov, Sergey and Baevski, Alexei and Fan, Angela and Gross, Sam and Ng, Nathan and Grangier, David and Auli, Michael},
    booktitle = {Proceedings of NAACL-HLT 2019: Demonstrations},
    date      = {2019},
    title     = {fairseq: A Fast, Extensible Toolkit for Sequence Modeling},
}

@inproceedings{Paszke2017Automatic,
    author    = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
    booktitle = {NIPS Autodiff Workshop},
    date      = {2017},
    title     = {Automatic Differentiation in {PyTorch}},
}

@inproceedings{Pennington2014Glove,
    author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
    url       = {http://www.aclweb.org/anthology/D14-1162},
    booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
    date      = {2014},
    pages     = {1532--1543},
    title     = {GloVe: Global Vectors for Word Representation},
}

@article{Ramisa2016BreakingNewsAA,
    author       = {Ramisa, Arnau and Yan, Fei and Moreno-Noguer, Francesc and Mikolajczyk, Krystian},
    date         = {2016},
    journaltitle = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    pages        = {1072--1085},
    title        = {BreakingNews: Article Annotation by Image and Text Processing},
    volume       = {40},
}

@inproceedings{Radford2019LanguageMA,
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    date   = {2019},
    title  = {Language Models are Unsupervised Multitask Learners},
}

@article{Rennie2016SelfCriticalST,
    author       = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
    date         = {2016},
    journaltitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages        = {1179--1195},
    title        = {Self-Critical Sequence Training for Image Captioning},
}

@article{Sennrich2015NeuralMT,
    author       = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
    date         = {2015},
    journaltitle = {ArXiv},
    title        = {Neural Machine Translation of Rare Words with Subword Units},
    volume       = {abs/1508.07909},
}

@article{Schroff2015FaceNetAU,
    author       = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
    date         = {2015},
    journaltitle = {2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages        = {815--823},
    title        = {FaceNet: A unified embedding for face recognition and clustering},
}

@article{Tariq2017ACE,
    author       = {Tariq, Amara and Foroosh, Hassan},
    date         = {2017},
    journaltitle = {IEEE transactions on image processing : a publication of the IEEE Signal Processing Society},
    pages        = {619--632},
    title        = {A Context-Driven Extractive Framework for Generating Realistic Image Descriptions.},
    volume       = {26 2},
}

@inproceedings{Vaswani2017AttentionIA,
    author    = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
    booktitle = {NIPS},
    date      = {2017},
    title     = {Attention Is All You Need},
}

@inproceedings{Wu2018PayLA,
    author    = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann and Auli, Michael},
    url       = {https://openreview.net/forum?id=SkVhlh09tX},
    booktitle = {International Conference on Learning Representations},
    date      = {2019},
    title     = {Pay Less Attention with Lightweight and Dynamic Convolutions},
}

@article{Xie2016AggregatedRT,
    author       = {Xie, Saining and Girshick, Ross B. and Doll{á}r, Piotr and Tu, Zhuowen and He, Kaiming},
    date         = {2016},
    journaltitle = {2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
    pages        = {5987--5995},
    title        = {Aggregated Residual Transformations for Deep Neural Networks},
}

@inproceedings{Xu2015ShowAA,
    author    = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron C. and Salakhutdinov, Ruslan and Zemel, Richard S. and Bengio, Yoshua},
    booktitle = {ICML},
    date      = {2015},
    title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
}

@article{Zhang2016JointFD,
    author       = {Zhang, Kaipeng and Zhang, Zhanpeng and Li, Zhifeng and Qiao, Yu},
    date         = {2016},
    journaltitle = {IEEE Signal Processing Letters},
    pages        = {1499--1503},
    title        = {Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks},
    volume       = {23},
}

