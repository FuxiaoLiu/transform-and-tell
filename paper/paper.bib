@inproceedings{Anonymous2020AreTU,
title={Are Transformers universal approximators of sequence-to-sequence functions?},
author={Anonymous},
booktitle={Submitted to International Conference on Learning Representations},
year={2020},
url={https://openreview.net/forum?id=ByxRM0Ntvr},
note={under review}
}

@InProceedings{Anderson2017BottomUpAT,
author = {Anderson, Peter and He, Xiaodong and Buehler, Chris and Teney, Damien and Johnson, Mark and Gould, Stephen and Zhang, Lei},
title = {Bottom-Up and Top-Down Attention for Image Captioning and Visual Question Answering},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2018}
}

@InProceedings{Biten2019GoodNews,
author = {Biten, Ali Furkan and Gomez, Lluis and Rusinol, Marcal and Karatzas, Dimosthenis},
title = {Good News, Everyone! Context Driven Entity-Aware Captioning for News Images},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@inproceedings{Bojar2018Findings,
    author    = {Bojar, Ond{ล}ej and Federmann, Christian and Fishel, Mark and Graham, Yvette and Haddow, Barry and Koehn, Philipp and Monz, Christof},
    location  = {Belgium, Brussels},
    publisher = {Association for Computational Linguistics},
    url       = {https://www.aclweb.org/anthology/W18-6401},
    booktitle = {Proceedings of the Third Conference on Machine Translation: Shared Task Papers},
    date      = {2018-10},
    doi       = {10.18653/v1/W18-6401},
    pages     = {272--303},
    title     = {Findings of the 2018 Conference on Machine Translation ({WMT}18)},
}

@article{Cao2017VGGFace2AD,
    author       = {Cao, Qiong and Shen, Li and Xie, Weidi and Parkhi, Omkar M. and Zisserman, Andrew},
    date         = {2017},
    journal = {2018 13th IEEE International Conference on Automatic Face \& Gesture Recognition (FG 2018)},
    pages        = {67--74},
    title        = {VGGFace2: A Dataset for Recognising Faces across Pose and Age},
}

@article{Chen2015MicrosoftCC,
    author       = {Chen, Xinlei and Fang, Hao and Lin, Tsung-Yi and Vedantam, Ramakrishna and Gupta, Saurabh and Doll{รก}r, Piotr and Zitnick, C. Lawrence},
    date         = {2015},
    journal = {ArXiv},
    title        = {Microsoft COCO Captions: Data Collection and Evaluation Server},
    volume       = {abs/1504.00325},
}

@InProceedings{Cornia2019ShowCT,
author = {Cornia, Marcella and Baraldi, Lorenzo and Cucchiara, Rita},
title = {Show, Control and Tell: A Framework for Generating Controllable and Grounded Captions},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2019}
}

@inproceedings{Devlin2019BERT,
    abstract  = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7{\%} (4.6{\%} absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
    author    = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
    location  = {Minneapolis, Minnesota},
    publisher = {Association for Computational Linguistics},
    booktitle = {Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)},
    date      = {2019-06},
    doi       = {10.18653/v1/N19-1423},
    pages     = {4171--4186},
    title     = {{BERT}: Pre-training of Deep Bidirectional Transformers for Language Understanding},
}

@InProceedings{Donahue2015LongTR,
author = {Donahue, Jeffrey and Anne Hendricks, Lisa and Guadarrama, Sergio and Rohrbach, Marcus and Venugopalan, Subhashini and Saenko, Kate and Darrell, Trevor},
title = {Long-Term Recurrent Convolutional Networks for Visual Recognition and Description},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@inproceedings{Fan2018HierarchicalNS,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}

@InProceedings{Fang2015FromCT,
author = {Fang, Hao and Gupta, Saurabh and Iandola, Forrest and Srivastava, Rupesh K. and Deng, Li and Dollar, Piotr and Gao, Jianfeng and He, Xiaodong and Mitchell, Margaret and Platt, John C. and Lawrence Zitnick, C. and Zweig, Geoffrey},
title = {From Captions to Visual Concepts and Back},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@ARTICLE{Feng2013AutomaticCG,
author       = {Feng, Yansong and Lapata, Mirella},
journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
title={Automatic Caption Generation for News Images},
year={2013},
volume={35},
number={4},
pages={797-812},
doi={10.1109/TPAMI.2012.118},
ISSN={},
month={April}
}

@inproceedings{Gao2019DeliberateAN,
  title={Deliberate Attention Networks for Image Captioning},
  author={Lianli Gao and Kaixuan Fan and Jingkuan Song and Xianglong Liu and Xing Xu and Heng Tao Shen},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  year={2019},
  pages     = {8320--8327},
}

@inproceedings{Gardner2017AllenNLP,
    title = "{A}llen{NLP}: A Deep Semantic Natural Language Processing Platform",
    author = "Gardner, Matt  and
      Grus, Joel  and
      Neumann, Mark  and
      Tafjord, Oyvind  and
      Dasigi, Pradeep  and
      Liu, Nelson F.  and
      Peters, Matthew  and
      Schmitz, Michael  and
      Zettlemoyer, Luke",
    booktitle = "Proceedings of Workshop for {NLP} Open Source Software ({NLP}-{OSS})",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-2501",
    doi = "10.18653/v1/W18-2501",
    pages = "1--6",
    abstract = "Modern natural language processing (NLP) research requires writing code. Ideally this code would provide a precise definition of the approach, easy repeatability of results, and a basis for extending the research. However, many research codebases bury high-level parameters under implementation details, are challenging to run and debug, and are difficult enough to extend that they are more likely to be rewritten. This paper describes AllenNLP, a library for applying deep learning methods to NLP research that addresses these issues with easy-to-use command-line tools, declarative configuration-driven experiments, and modular NLP abstractions. AllenNLP has already increased the rate of research experimentation and the sharing of NLP components at the Allen Institute for Artificial Intelligence, and we are working to have the same impact across the field.",
}

@InProceedings{Grave2016EfficientSA,
  title = 	 {Efficient softmax approximation for {GPU}s},
  author = 	 {{\'E}douard Grave and Armand Joulin and Moustapha Ciss{\'e} and David Grangier and Herv{\'e} J{\'e}gou},
  booktitle = 	 {Proceedings of the 34th International Conference on Machine Learning},
  pages = 	 {1302--1310},
  year = 	 {2017},
  editor = 	 {Doina Precup and Yee Whye Teh},
  volume = 	 {70},
  series = 	 {Proceedings of Machine Learning Research},
  address = 	 {International Convention Centre, Sydney, Australia},
  month = 	 {06--11 Aug},
  publisher = 	 {PMLR},
  pdf = 	 {http://proceedings.mlr.press/v70/grave17a/grave17a.pdf},
  url = 	 {http://proceedings.mlr.press/v70/grave17a.html},
  abstract = 	 {We propose an approximate strategy to efficiently train neural network based language models over very large vocabularies. Our approach, called adaptive softmax, circumvents the linear dependency on the vocabulary size by exploiting the unbalanced word distribution to form clusters that explicitly minimize the expectation of computation time. Our approach further reduces the computational cost by exploiting the specificities of modern architectures and matrix-matrix vector operations, making it particularly suited for graphical processing units. Our experiments carried out on standard benchmarks, such as EuroParl and One Billion Word, show that our approach brings a large gain in efficiency over standard approximations while achieving an accuracy close to that of the full softmax. The code of our method is available at https://github.com/facebookresearch/adaptive-softmax.}
}

@InProceedings{Karpathy2015DeepVA,
author = {Karpathy, Andrej and Fei-Fei, Li},
title = {Deep Visual-Semantic Alignments for Generating Image Descriptions},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@incollection{Kim2016MultimodalRL,
title = {Multimodal Residual Learning for Visual QA},
author = {Kim, Jin-Hwa and Lee, Sang-Woo and Kwak, Donghyun and Heo, Min-Oh and Kim, Jeonghee and Ha, Jung-Woo and Zhang, Byoung-Tak},
booktitle = {Advances in Neural Information Processing Systems 29},
editor = {D. D. Lee and M. Sugiyama and U. V. Luxburg and I. Guyon and R. Garnett},
pages = {361--369},
year = {2016},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/6446-multimodal-residual-learning-for-visual-qa.pdf}
}

@article{Lample2019CrosslingualLM,
    author       = {Lample, Guillaume and Conneau, Alexis},
    date         = {2019},
    journal = {ArXiv},
    title        = {Cross-lingual Language Model Pretraining},
    volume       = {abs/1901.07291},
}

@article{Lan2019ALBERT,
    author       = {Lan, Zhen-Zhong and Chen, Mingda and Goodman, Sebastian and Gimpel, Kevin and Sharma, Piyush and Soricut, Radu},
    date         = {2019},
    journal = {ArXiv},
    title        = {ALBERT: A Lite BERT for Self-supervised Learning of Language Representations},
    volume       = {abs/1909.11942},
}

@article{Li2019Boosted,
    author       = {Li, Jiangyun and Yao, Peng and Guo, Longteng and Zhang, Weicun},
    publisher    = {Multidisciplinary Digital Publishing Institute},
    date         = {2019},
    journal = {Applied Sciences},
    number       = {16},
    pages        = {3260},
    title        = {Boosted Transformer for Image Captioning},
    volume       = {9},
}

@inproceedings{Lin2014MicrosoftCC,
    author    = {Lin, Tsung-Yi and Maire, Michael and Belongie, Serge J. and Bourdev, Lubomir D. and Girshick, Ross B. and Hays, James and Perona, Pietro and Ramanan, Deva and Doll{รก}r, Piotr and Zitnick, C. Lawrence},
    booktitle = {ECCV},
    date      = {2014},
    title     = {Microsoft COCO: Common Objects in Context},
}

@article{Liu2019RoBERTaAR,
    author       = {Liu, Yinhan and Ott, Myle and Goyal, Naman and Du, Jingfei and Joshi, Mandar S. and Chen, Danqi and Levy, Omer and Lewis, Mike and Zettlemoyer, Luke S. and Stoyanov, Veselin},
    date         = {2019},
    journal = {ArXiv},
    title        = {RoBERTa: A Robustly Optimized BERT Pretraining Approach},
    volume       = {abs/1907.11692},
}

@InProceedings{Lu2017KnowingWT,
author = {Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
title = {Knowing When to Look: Adaptive Attention via a Visual Sentinel for Image Captioning},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{Lu2018EntityAI,
    title = "Entity-aware Image Caption Generation",
    author = "Lu, Di  and
      Whitehead, Spencer  and
      Huang, Lifu  and
      Ji, Heng  and
      Chang, Shih-Fu",
    booktitle = "Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing",
    month = oct # "-" # nov,
    year = "2018",
    address = "Brussels, Belgium",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D18-1435",
    doi = "10.18653/v1/D18-1435",
    pages = "4013--4023",
    abstract = "Current image captioning approaches generate descriptions which lack specific information, such as named entities that are involved in the images. In this paper we propose a new task which aims to generate informative image captions, given images and hashtags as input. We propose a simple but effective approach to tackle this problem. We first train a convolutional neural networks - long short term memory networks (CNN-LSTM) model to generate a template caption based on the input image. Then we use a knowledge graph based collective inference algorithm to fill in the template with specific named entities retrieved via the hashtags. Experiments on a new benchmark dataset collected from Flickr show that our model generates news-style image descriptions with much richer information. Our model outperforms unimodal baselines significantly with various evaluation metrics.",
}

@incollection{Mikolov2013DistributedRO,
title = {Distributed Representations of Words and Phrases and their Compositionality},
author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg S and Dean, Jeff},
booktitle = {Advances in Neural Information Processing Systems 26},
editor = {C. J. C. Burges and L. Bottou and M. Welling and Z. Ghahramani and K. Q. Weinberger},
pages = {3111--3119},
year = {2013},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf}
}

@inproceedings{Ott2019Fairseq,
    title = "fairseq: A Fast, Extensible Toolkit for Sequence Modeling",
    author = "Ott, Myle  and
      Edunov, Sergey  and
      Baevski, Alexei  and
      Fan, Angela  and
      Gross, Sam  and
      Ng, Nathan  and
      Grangier, David  and
      Auli, Michael",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics (Demonstrations)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-4009",
    doi = "10.18653/v1/N19-4009",
    pages = "48--53",
    abstract = "fairseq is an open-source sequence modeling toolkit that allows researchers and developers to train custom models for translation, summarization, language modeling, and other text generation tasks. The toolkit is based on PyTorch and supports distributed training across multiple GPUs and machines. We also support fast mixed-precision training and inference on modern GPUs. A demo video can be found at https://www.youtube.com/watch?v=OtgDdWtHvto",
}

@inproceedings{Paszke2017Automatic,
    author    = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
    booktitle = {NIPS Autodiff Workshop},
    date      = {2017},
    title     = {Automatic Differentiation in {PyTorch}},
}

@inproceedings{Pennington2014Glove,
    author    = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher D.},
    url       = {http://www.aclweb.org/anthology/D14-1162},
    booktitle = {Empirical Methods in Natural Language Processing (EMNLP)},
    date      = {2014},
    pages     = {1532--1543},
    title     = {GloVe: Global Vectors for Word Representation},
}

@inproceedings{Radford2019LanguageMA,
    author = {Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
    date   = {2019},
    title  = {Language Models are Unsupervised Multitask Learners},
}

@inproceedings{Rajpurkar2016SQuAD,
    author    = {Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
    location  = {Austin, Texas},
    publisher = {Association for Computational Linguistics},
    url       = {https://www.aclweb.org/anthology/D16-1264},
    booktitle = {Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing},
    date      = {2016-11},
    doi       = {10.18653/v1/D16-1264},
    pages     = {2383--2392},
    title     = {{SQ}u{AD}: 100,000+ Questions for Machine Comprehension of Text},
}

@inproceedings{Rajpurkar2018KnowWY,
    abstract  = {Extractive reading comprehension systems can often locate the correct answer to a question in a context document, but they also tend to make unreliable guesses on questions for which the correct answer is not stated in the context. Existing datasets either focus exclusively on answerable questions, or use automatically generated unanswerable questions that are easy to identify. To address these weaknesses, we present SQuADRUn, a new dataset that combines the existing Stanford Question Answering Dataset (SQuAD) with over 50,000 unanswerable questions written adversarially by crowdworkers to look similar to answerable ones. To do well on SQuADRUn, systems must not only answer questions when possible, but also determine when no answer is supported by the paragraph and abstain from answering. SQuADRUn is a challenging natural language understanding task for existing models: a strong neural system that gets 86{\%} F1 on SQuAD achieves only 66{\%} F1 on SQuADRUn. We release SQuADRUn to the community as the successor to SQuAD.},
    author    = {Rajpurkar, Pranav and Jia, Robin and Liang, Percy},
    location  = {Melbourne, Australia},
    publisher = {Association for Computational Linguistics},
    url       = {https://www.aclweb.org/anthology/P18-2124},
    booktitle = {Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)},
    date      = {2018-07},
    doi       = {10.18653/v1/P18-2124},
    pages     = {784--789},
    title     = {Know What You Don{'}t Know: Unanswerable Questions for {SQ}u{AD}},
}

@article{Ramisa2016BreakingNewsAA,
    author       = {Ramisa, Arnau and Yan, Fei and Moreno-Noguer, Francesc and Mikolajczyk, Krystian},
    date         = {2016},
    journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
    pages        = {1072--1085},
    title        = {BreakingNews: Article Annotation by Image and Text Processing},
    volume       = {40},
}

@inproceedings{Ren2015FasterRCNN,
    author    = {Ren, Shaoqing and He, Kaiming and Girshick, Ross and Sun, Jian},
    booktitle = {Advances in neural information processing systems},
    date      = {2015},
    pages     = {91--99},
    title     = {Faster r-cnn: Towards real-time object detection with region proposal networks},
}
@InProceedings{Rennie2017SelfCriticalST,
author = {Rennie, Steven J. and Marcheret, Etienne and Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
title = {Self-Critical Sequence Training for Image Captioning},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{Sennrich2015NeuralMT,
    author    = {Sennrich, Rico and Haddow, Barry and Birch, Alexandra},
    location  = {Berlin, Germany},
    publisher = {Association for Computational Linguistics},
    url       = {https://www.aclweb.org/anthology/P16-1162},
    booktitle = {Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    date      = {2016-08},
    doi       = {10.18653/v1/P16-1162},
    pages     = {1715--1725},
    title     = {Neural Machine Translation of Rare Words with Subword Units},
}


@InProceedings{Schroff2015FaceNetAU,
author = {Schroff, Florian and Kalenichenko, Dmitry and Philbin, James},
title = {FaceNet: A Unified Embedding for Face Recognition and Clustering},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@inproceedings{Sharma2018ConceptualCA,
    title = "Conceptual Captions: A Cleaned, Hypernymed, Image Alt-text Dataset For Automatic Image Captioning",
    author = "Sharma, Piyush  and
      Ding, Nan  and
      Goodman, Sebastian  and
      Soricut, Radu",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1238",
    doi = "10.18653/v1/P18-1238",
    pages = "2556--2565",
    abstract = "We present a new dataset of image caption annotations, Conceptual Captions, which contains an order of magnitude more images than the MS-COCO dataset (Lin et al., 2014) and represents a wider variety of both images and image caption styles. We achieve this by extracting and filtering image caption annotations from billions of webpages. We also present quantitative evaluations of a number of image captioning models and show that a model architecture based on Inception-ResNetv2 (Szegedy et al., 2016) for image-feature extraction and Transformer (Vaswani et al., 2017) for sequence modeling achieves the best performance when trained on the Conceptual Captions dataset.",
}

@article{Subramanian2019OnEA,
    author       = {Subramanian, Sandeep and Li, Raymond and Pilault, Jonathan and Pal, Christopher Joseph},
    date         = {2019},
    journal = {ArXiv},
    title        = {On Extractive and Abstractive Neural Document Summarization with Transformer Language Models},
    volume       = {abs/1909.03186},
}

@ARTICLE{Tariq2017ACE,
author={A. {Tariq} and H. {Foroosh}},
journal={IEEE Transactions on Image Processing},
title={A Context-Driven Extractive Framework for Generating Realistic Image Descriptions},
year={2017},
volume={26},
number={2},
pages={619-632},
keywords={image annotation;image retrieval;meta data;probability;context-driven extractive framework;realistic image description generation;automatic image annotation;image search;image retrieval;organization systems;semantic concepts;visual features;semantic gap;annotation systems;contextual cues;artificial ground truth descriptions;data collection;image descriptions;real-world data set;news image captions;auxiliary information sources;metadata;probability space;annotation prediction;Context;Visualization;Training;Semantics;Estimation;Adaptation models;Vocabulary;Textual image description;context discovery;image semantics;heterogeneous information fusion},
doi={10.1109/TIP.2016.2628585},
ISSN={},
month={Feb},}

@incollection{Vaswani2017AttentionIA,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}


@InProceedings{Vinyals2015ShowAT,
author = {Vinyals, Oriol and Toshev, Alexander and Bengio, Samy and Erhan, Dumitru},
title = {Show and Tell: A Neural Image Caption Generator},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2015}
}

@inproceedings{Wang2019GLUE,
    author    = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
    url       = {https://openreview.net/forum?id=rJ4km2R5t7},
    booktitle = {International Conference on Learning Representations},
    date      = {2019},
    title     = {{GLUE}: A Multi-Task Benchmark and Analysis Platform for Natural Language Understanding},
}

@article{Wang2019SuperGLUEAS,
    author       = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
    date         = {2019},
    journal = {ArXiv},
    title        = {SuperGLUE: A Stickier Benchmark for General-Purpose Language Understanding Systems},
    volume       = {abs/1905.00537},
}

@inproceedings{Wang2019Hierarchical,
    author    = {Wang, Weixuan and Chen, Zhihong and Hu, Haifeng},
    booktitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
    date      = {2019},
    pages     = {8957--8964},
    title     = {Hierarchical attention network for image captioning},
    volume    = {33},
}

@inproceedings{Wu2018PayLA,
    author    = {Wu, Felix and Fan, Angela and Baevski, Alexei and Dauphin, Yann and Auli, Michael},
    url       = {https://openreview.net/forum?id=SkVhlh09tX},
    booktitle = {International Conference on Learning Representations},
    date      = {2019},
    title     = {Pay Less Attention with Lightweight and Dynamic Convolutions},
}


@InProceedings{Xie2017AggregatedRT,
author = {Xie, Saining and Girshick, Ross and Dollar, Piotr and Tu, Zhuowen and He, Kaiming},
title = {Aggregated Residual Transformations for Deep Neural Networks},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {July},
year = {2017}
}

@inproceedings{Xu2015ShowAA,
    author    = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron C. and Salakhutdinov, Ruslan and Zemel, Richard S. and Bengio, Yoshua},
    booktitle = {ICML},
    date      = {2015},
    title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
}

@article{Yang2019XLNetGA,
    author       = {Yang, Zhilin and Dai, Zihang and Yang, Yiming and Carbonell, Jaime G. and Salakhutdinov, Ruslan and Le, Quoc V.},
    date         = {2019},
    journal = {ArXiv},
    title        = {XLNet: Generalized Autoregressive Pretraining for Language Understanding},
    volume       = {abs/1906.08237},
}

@InProceedings{You2016ImageCW,
author = {You, Quanzeng and Jin, Hailin and Wang, Zhaowen and Fang, Chen and Luo, Jiebo},
title = {Image Captioning With Semantic Attention},
booktitle = {The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)},
month = {June},
year = {2016}
}

@article{Young2014FromID,
    title = "From image descriptions to visual denotations: New similarity metrics for semantic inference over event descriptions",
    author = "Young, Peter  and
      Lai, Alice  and
      Hodosh, Micah  and
      Hockenmaier, Julia",
    journal = "Transactions of the Association for Computational Linguistics",
    volume = "2",
    year = "2014",
    url = "https://www.aclweb.org/anthology/Q14-1006",
    doi = "10.1162/tacl_a_00166",
    pages = "67--78",
    abstract = "We propose to use the visual denotations of linguistic expressions (i.e. the set of images they describe) to define novel denotational similarity metrics, which we show to be at least as beneficial as distributional similarities for two tasks that require semantic inference. To compute these denotational similarities, we construct a denotation graph, i.e. a subsumption hierarchy over constituents and their denotations, based on a large corpus of 30K images and 150K descriptive captions.",
}

@article{Zhang2016JointFD,
    author       = {Zhang, Kaipeng and Zhang, Zhanpeng and Li, Zhifeng and Qiao, Yu},
    date         = {2016},
    journal = {IEEE Signal Processing Letters},
    pages        = {1499--1503},
    title        = {Joint Face Detection and Alignment Using Multitask Cascaded Convolutional Networks},
    volume       = {23},
}

@inproceedings{Zhao2019InformativeIC,
    title = "Informative Image Captioning with External Sources of Information",
    author = "Zhao, Sanqiang  and
      Sharma, Piyush  and
      Levinboim, Tomer  and
      Soricut, Radu",
    booktitle = "Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2019",
    address = "Florence, Italy",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P19-1650",
    doi = "10.18653/v1/P19-1650",
    pages = "6485--6494",
    abstract = "An image caption should fluently present the essential information in a given image, including informative, fine-grained entity mentions and the manner in which these entities interact. However, current captioning models are usually trained to generate captions that only contain common object names, thus falling short on an important {``}informativeness{''} dimension. We present a mechanism for integrating image information together with fine-grained labels (assumed to be generated by some upstream models) into a caption that describes the image in a fluent and informative manner. We introduce a multimodal, multi-encoder model based on Transformer that ingests both image features and multiple sources of entity labels. We demonstrate that we can learn to control the appearance of these entity labels in the output, resulting in captions that are both fluent and informative.",
}

@article{Zhu2018CaptioningTW,
    author       = {Zhu, Xinxin and Li, Lixiang and Liu, Jing and Peng, Haipeng and Niu, Xinxin},
    publisher    = {Multidisciplinary Digital Publishing Institute},
    date         = {2018},
    journal = {Applied Sciences},
    number       = {5},
    pages        = {739},
    title        = {Captioning transformer with stacked attention modules},
    volume       = {8},
}

