% !TEX root = main.tex

\section{Model Architecture}

\subsection{Encoders}

Our proposed model takes three inputs: the image, the faces, and article text.
Each of these inputs first goes through an encoder to give us a vector
representation.

\subsubsection{Image Embedding}

For the image, we feed it through a pretrained ResNet-152 model
\cite{He2016ResNet} and use the output of the final block, just before the
pooling layer, as the image embedding $\bX_I \in \mathbb{R}^{2048 \times 49}$.
The embedding forms a 7 by 7 block, allowing the transformer to attend to 49
different patches in the image.

\subsubsection{Article Embedding}

For the article, we use RoBERTa \cite{Liu2019RoBERTaAR} to encode the text.
RoBERTa, a more carefully trained BERT \cite{Devlin2019BERT}, is a language
representation model that provides pretrained contextual embeddings for text.
Unlike GloVe \cite{Pennington2014Glove} and word2vec
\cite{Mikolov2013DistributedRO} embeddings, where each word has exactly one
representation, the bidirectionality and the attention mechanism in the
transformer allow a word to have different vector representations depending on
the surrounding context.

The largest GloVe model has a vocabulary size of 1.2 million. Although this is
large, many rare names will still get mapped to the unknown token. In contrast,
RoBERTa uses BPE \cite{Sennrich2015NeuralMT,Radford2019LanguageMA} which can
encode any word that can be written in Unicode characters.

One limitation of RoBERTa is that the maximum length of the input sequence is
512. For GoodNews, we simply encode the first 512 tokens of the article. For
NYTimes800k, since we have the image position, we concatenate the title, the
first paragraph, and as many paragraphs above and below the image as we can
fit, until we reach the 512 token limit. Note that since we are using BPE, a
word might consist of many tokens. On average, we can only encode ...... words
of the article.

The RoBERTa encoder provides gives us the article embedding $\bX_T \in
\mathbb{R}^{1024 \times S}$ where $S$ is the number of tokens.


\subsubsection{Face Embedding}

To embed the faces, we first use MTCNN \cite{Zhang2016JointFD} to detect the
face bounding boxes. We then select the top $M$ faces and feed them through a
FaceNet model \cite{Schroff2015FaceNetAU}, pretrained on the VGGFace2 dataset
\cite{Cao2017VGGFace2AD}, to obtain a face embedding $\bX_F \in
\mathbb{R}^{512 \times M}$ where $M$ is the number of faces.

\subsection{Decoder}

The decoder is a function that estimates $p(y_t)$, the probability of the next
token, conditional on the past $\by_{<t}$ and the context embeddings $\bX_I$,
$\bX_T$, and $\bX_F$:
\begin{IEEEeqnarray*}{lCl}
   p(y_t) &=& \mathbb{P}(Y_t = y_t \mid \by_{<t}, \bX_I, \bX_T, \bX_F)
\end{IEEEeqnarray*}
In our architecture, the decoder consists of four layers of transformer blocks.
In each block, the conditioning on past tokens is computed using dynamic
convolutions \cite{Wu2018PayLA}, and the conditioning on the contexts is
computed using multi-head attention \cite{Vaswani2017AttentionIA}.


\subsubsection{Multi-Head Attention}

Let $\bh_t \in \mathbb{R}^{1024}$ be the current hidden state when decoding the
$t$th token. We divide the embedding dimension into 16 attention heads, each
with size 64. For each head $i \in \{1, 2, ..., 16\}$, we first do a linear
projection of $\bh_t$ and the image embedding $\bX_I$ into a query $\bq_{It}
\in \mathbb{R}^{64}$, key $\bK_I \in \mathbb{R}^{64 \times 49}$, and value
$\bV_I \in \mathbb{R}^{64 \times 49}$:
\begin{IEEEeqnarray*}{lCl}
   \bq_{Iit} &=& \bW_{Iiq} \, \bh_t \\
   \bK_{Ii} &=& \bW_{Iik} \, \bX_I \\
   \bV_{Ii} &=& \bW_{Iiv} \, \bX_I
\end{IEEEeqnarray*}
Then the attended image for each head $\bx'_{Ii} \in \mathbb{R}^{64}$ is the
weighted sum of the values, where the weights are obtained from the dot product
between the query and key:
\begin{IEEEeqnarray*}{lCl}
   \bx'_{Ii} &=& \text{softmax}\left(\bK_{Ii}^T \bq_{Iit} \right) \bV_{Ii}
\end{IEEEeqnarray*}
The attention from each head is then concatenated into $\bx'_{I} \in
\mathbb{R}^{1024}$:
\begin{IEEEeqnarray*}{lCl}
   \bx'_{I} &=& [\tx_{I1}, \tx_{I2}, ..., \tx_{I16}]
\end{IEEEeqnarray*}
and the overall image attention $\tx_{I} \in \mathbb{R}^{1024}$ is obtained
after adding a residual connection and layer normalization:
\begin{IEEEeqnarray*}{lCl}
   \tx_{I} &=& \text{LayerNorm}(\bh_t + \bx'_{I})
\end{IEEEeqnarray*}
We use the same attention mechanism (with different weight matrices) to obtain
the attended article $\tx_T$ and the attended face $\tx_F$. These three are
then concatenated and fed through a feedforward layer:
\begin{IEEEeqnarray*}{lCl}
   \tx_C &=& [\tx_I, \tx_T, \tx_F] \\
   \tx_R &=& \bW_C \, \tx_C + \bb_C \\
   \tx_D &=& \text{ReLU}(\bW_R \, \tx_R + \bb_R )\\
   \tx_E &=& \text{LayerNorm}(\tx_R + \bW_D \, \tx_D + \bb_D)
\end{IEEEeqnarray*}
The final output $\tx_E \in \mathbb{R}^{1024}$ is used as the input to the
next transformer block.

\subsubsection{Dynamic Convolutions}

Instead of using the standard self-attention module as in current
state-of-the-art GPT-2 architecture \cite{Radford2019LanguageMA}, we find that
dynamic convolutions \cite{Wu2018PayLA} are more efficient to train, especially
when there is limited GPU compute power.

Given an input $\bz_t \in \mathbb{R}^{1024}$, which can either be the input
embedding of the previous token, or the output from the previous transformer
block, we first use a feedforward layer with a gated linear unit (GLU)
\cite{Dauphin2017GLU} as the activation function:
\begin{IEEEeqnarray*}{lCl}
   \bz_t' &=& \text{GLU}(\bW_Z \, \bz_t + \bb_Z)
\end{IEEEeqnarray*}


\subsection{Bag of Tricks}

\subsubsection{Mixing RoBERTa layers}
RoBERTa consists of 24 layers of bidirectional transformer blocks. Given an
input of length $S$, the pretrained RoBERTa encoder will return 25 sequences of
embeddings, $\bG_i \in \mathbb{R}^{2048 \times S}$ for $i \in \{0,1,
2,...,24\}$. This includes the initial uncontextualized embeddings and the
output of each of the 24 layers. Inspired by Tenney \etal
\cite{Tenney2019BertRT}, who showed that different layers in BERT represent
different steps in the traditional NLP pipeline, we take a weighted sum across
all layers to obtain the article embedding:
\begin{IEEEeqnarray*}{lCl}
   \bX_T &=& \sum_{i=0}^{24} \alpha_i \bG_i
\end{IEEEeqnarray*}
where $\bx_T \in \mathbb{R}^{1024 \times S}$ is the article embedding
and $\alpha_i$ are learnable weights.

\subsubsection{Copying with Multi-headed Attention}

Inspired by pointer-generator networks \cite{See2017GetTT}, we introduce
a copying mechanism using multi-headed attention.

\subsubsection{Adaptive Softmax}

The BPE vocabulary size is 50265. To make training more efficient, we use adaptive softmax
\cite{Grave2016EfficientSA} and divide the vocabulary into three clusters: 5K,
15K, and 25K. We tie the adaptive weights and we share the decoder input and
output embeddings. We use sinusoidal positional encoding
\cite{Vaswani2017AttentionIA} to represent the position of each token.
