% !TEX root = main.tex

\section{Model Architecture}

Conceptually our model can be broken into two parts: encoding and decoding. The
encoding part consists of a set of domain specific encoders for producing high
level vector representations of images, faces and article text. The output of
each encoder is a, potentially arbitrary, length set of fixed size vectors that
represent the input. The decoder sequentially generates captions at the
sub-word level by applying
multi-headed attention over the sets of vectors from the encoders, and over a
representation of the previously generated sub-word units. In
practice there are a number details which allow us to train this large
multi-faceted model on a single machine and achieve state-of-the-art
performance. We have included some these details where appropriate and
collected together those that do not fit elsewhere into
Section~\ref{ssec:bag_of_tricks}.

\subsection{Encoders}

Our proposed model takes three types of inputs: image, faces, and article text.
Each of these inputs is encoded into a set of vectors by domain specific
encoders pre-trained on data from the matching domain.

\subsubsection{Image Encoder}

A high level image representation is obtained with a
ResNet-152~\cite{He2016ResNet} model pre-trained on ImageNet. We use the output
of the final block before the pooling layer as the image representation. This
is a set of 49 different vectors $\bx_{Ii} \in \mathbb{R}^{2048}$ where each
vector corresponds to a separate image patch after the image is divided into
equal size 7 by 7 patches. Using this representation $\bX_I = \{\bx_{Ii} \in
\mathbb{R}^{2048}\}_{i=1}^{49}$ allows the decoder to attend different regions
in the image---a modeling choice that has proven useful in other image
captioning tasks~\cite{Xu2015ShowAA}.

\subsubsection{Face Encoder}

We use MTCNN~\cite{Zhang2016JointFD} to detect face bounding boxes in the
image. We then select the largest four faces since the majority of captions
have at most four personal names (see~\ref{ssec:nytimes800k}). A vector
representation of each face is obtained by passing the bounding boxes to
FaceNet~\cite{Schroff2015FaceNetAU}, which was pre-trained on the VGGFace2
dataset~\cite{Cao2017VGGFace2AD}. The resulting set of face vectors for each
image is $\bx_{F} = \{\bx_{Fi} \in \mathbb{R}^{512}\}_{i=1}^4$.

Even though the faces are extracted from the image it is useful to consider
them as an input domain that is separate to the image. This is because
specialised models are needed to make full use of them.

%them to be a separate
%input domain since face bounding boxes are extracted as a pre-processing step
%and we use a specialised model to encode features from each bounding box.

\eat{We then select the top $M$ faces and feed them through a
FaceNet model~\cite{Schroff2015FaceNetAU}, pretrained on the VGGFace2 dataset
\cite{Cao2017VGGFace2AD}, to obtain a face embedding $\bx_{F} =
\{\bx_{F,i} \in \mathbb{R}^{512}\}_{i=0}^M$.}

\subsubsection{Article Encoder}

To encode the article text we use RoBERTa~\cite{Liu2019RoBERTaAR} which is a
recent improvement over the popular BERT~\cite{Devlin2019BERT} model.
RoBERTa is a language representation model that provides pretrained contextual
embeddings for text. It consists of 24 layers of bidirectional transformer
blocks.
% It is a more carefully trained BERT~\cite{Devlin2019BERT} model.

Unlike GloVe~\cite{Pennington2014Glove} and word2vec
\cite{Mikolov2013DistributedRO} embeddings, where each word has exactly one
representation, the bidirectionality and the attention mechanism in the
transformer allow a word to have different vector representations depending on
the surrounding context.

The largest GloVe model has a vocabulary size of 1.2 million. Although this is
large, many rare names will still get mapped to the unknown token. In contrast,
RoBERTa uses BPE~\cite{Sennrich2015NeuralMT,Radford2019LanguageMA} which can
encode any word that can be written in Unicode characters.

One limitation of RoBERTa is that the maximum length of the input sequence is
512. For GoodNews, we simply encode the first 512 tokens of the article. For
NYTimes800k, since we have the image position, we concatenate the title, the
first paragraph, and as many paragraphs above and below the image as we can
fit, until we reach the 512 token limit. Note that since we are using BPE, a
word might consist of many tokens. On average, \verify{we can only encode
...... words of the article.}

The RoBERTa encoder provides gives us the set of token embeddings $\bX_{T} =
\{\bx_{Ti} \in \mathbb{R}^{1024}\}_{i=1}^S$, where $S$ is the number of tokens.


\subsection{Decoder}

The decoder is a function that estimates $p(y_t)$, the probability of the $t$th
token in the caption, conditional on the past $\by_{<t}$ and the context
embeddings $\bX_I$, $\bX_T$, and $\bX_F$:
\begin{IEEEeqnarray*}{lCl}
   p(y_t) &=& \mathbb{P}(Y_t = y_t \mid \by_{<t}, \bX_I, \bX_T, \bX_F)
\end{IEEEeqnarray*}
In our architecture, the decoder consists of four transformer blocks. In each
block, the conditioning on past tokens is computed using dynamic convolutions
\cite{Wu2018PayLA}, and the conditioning on the contexts is computed using
multi-head attention~\cite{Vaswani2017AttentionIA}.


\subsubsection{Dynamic Convolutions}

Instead of using the standard self-attention module as in current
state-of-the-art GPT-2 decoder~\cite{Radford2019LanguageMA}, we find that
dynamic convolutions~\cite{Wu2018PayLA} are more efficient to train. Suppose
when decoding the $t$th token, we have at the $\ell$th block the input
$\bz_{\ell t} \in \mathbb{R}^{1024}$. If $\ell = 0$, then $\bz_{0t}$ is the
embedding of the previous token. Otherwise it is the output from the previous
transformer block. Given kernel size $K$ and 16 attention heads, for each head
$h \in \{1, 2, ..., 16\}$, we first project the current and last $K-1$ steps
using a feedforward layer:
\begin{IEEEeqnarray*}{lCl}
   \bz_{\ell,h,t-j}' &=& \text{GLU}(\bW_{z\ell h} \, \bz_{\ell,t-j} + \bb_{z\ell h})
\end{IEEEeqnarray*}
where $j \in \{0,1,...,K-1\}$, GLU is the gated linear unit activation
function~\cite{Dauphin2017GLU}, and $\bz_{\ell,h,t-j}' \in \mathbb{R}^{64}$.
The output of each head's dynamic convolution is the weighted sum of these
projected values:
\begin{IEEEeqnarray*}{lCl}
   \tz_{\ell h t} &=& \sum_{j=0}^{K-1} \gamma_{\ell hj} \, \bz_{\ell, h, t-j}'
\end{IEEEeqnarray*}
where the weight $\gamma_{\ell hj}$ is a linear projection of the input,
followed by a softmax over the kernel window:
\begin{IEEEeqnarray*}{lCl}
   \gamma_{\ell hj} &=& \text{Softmax} \left( \bw^T_{\gamma \ell h} \,
      \bz'_{\ell,h,t-j} \right)
\end{IEEEeqnarray*}
The overall output is the concatenation of all the head outputs, followed by
a feedforward with a residual connection and layer normalization:
\begin{IEEEeqnarray*}{lCl}
   \tz_{\ell t} &=& [ \tz_{\ell 1 t}, \tz_{\ell 2 t},..., \tz_{\ell 16 t} ] \\
   \bd_{\ell t} &=& \text{LayerNorm}\left( \bz_{\ell t} +
             \bW_{\tz \ell} \, \tz_{\ell t} + \bb_{\tz \ell} \right)
\end{IEEEeqnarray*}
Note that given kernel size $K$, we can attend to the current time step and the
last $K-1$ steps. Following closely to~\cite{Wu2018PayLA}, our decoder has 4
transformer blocks with kernel sizes 3, 7, 15, and 31, respectively. Thus the
final block output will have collected information from the last 51 tokens.

\subsubsection{Multi-Head Attention}
\label{ssection:attn}

Given $\bd_{\ell t} \in \mathbb{R}^{1024}$, the output of the dynamic
convolution at layer $\ell$, we can now attend over the image context using
multi-head attention, also with 16 heads. For each head $h \in \{1, 2, ...,
16\}$, we first do a linear projection of $\bd_{\ell t}$ and the image
embeddings $\bX_I$ into a query $\bq_{I \ell h t} \in \mathbb{R}^{64}$, a set
of keys $\bK_{I \ell h t} = \{\bk_{I \ell h t i} \in \mathbb{R}^{64}
\}_{i=1}^{49}$, and the corresponding values $\bV_{I \ell h t} = \{ \bv_{I \ell
h t i} \in \mathbb{R}^{64} \}_{i=1}^{49}$:
\begin{IEEEeqnarray*}{lCl}
   \bq_{I\ell ht} &=& \bW_{I \ell h q} \, \bd_{\ell t} \\
   \bk_{I\ell h i} &=& \bW_{I \ell h k} \, \bx_{I i}
      \qquad \forall i \in \{1, 2, ..., 49\}\\
   \bv_{I \ell h i} &=& \bW_{I \ell h v} \, \bx_{I i}
      \qquad \forall i \in \{1, 2, ..., 49\}
\end{IEEEeqnarray*}
Then the attended image for each head is the weighted sum of the values, where
the weights are obtained from the dot product between the query and key:
\begin{IEEEeqnarray*}{lCl}
   \lambda_{I \ell h i} &=&\text{softmax}\left(\bk_{I\ell h i}^T \bq_{I \ell ht} \right)\\
   \bx'_{I \ell h t} &=& \sum_{i = 1}^{49}
      \lambda_{I \ell h i} \, \bv_{I \ell h i}
\end{IEEEeqnarray*}
The attention from each head is then concatenated into $\bx'_{I\ell t} \in
\mathbb{R}^{1024}$:
\begin{IEEEeqnarray*}{lCl}
   \bx'_{I\ell t} &=& [\tx_{I\ell 1 t}, \tx_{I\ell 2 t}, ..., \tx_{I\ell 16 t}]
\end{IEEEeqnarray*}
and the overall image attention $\tx_{I\ell t} \in \mathbb{R}^{1024}$ is obtained
after adding a residual connection and layer normalization:
\begin{IEEEeqnarray*}{lCl}
   \tx_{I \ell t} &=& \text{LayerNorm}(\bd_{\ell t} + \bx'_{I\ell t})
\end{IEEEeqnarray*}
We use the same multi-head attention mechanism (with different weight matrices)
to obtain the attended article $\tx_{T \ell t}$ and the attended face $\tx_{F
\ell t}$. These three are finally concatenated and fed through a feedforward
layer:
\begin{IEEEeqnarray*}{lCl}
   \tx_{C \ell t} &=& [\tx_{I \ell t}, \tx_{T \ell t}, \tx_{F \ell t}] \\
   \tx_{R \ell t} &=& \bW_{C \ell} \, \tx_{C \ell t} + \bb_{C \ell} \\
   \tx_{D \ell t} &=& \text{ReLU}(\bW_{R \ell} \, \tx_{R \ell t} + \bb_{R \ell} )\\
   \bz_{\ell + 1,t} &=& \text{LayerNorm}(\tx_{R \ell t} + \bW_{D \ell} \,
      \tx_{D \ell t} + \bb_{D \ell})
\end{IEEEeqnarray*}
The final output $\bz_{\ell + 1,t} \in \mathbb{R}^{1024}$ is used as the input
to the next transformer block, or if we are in the last block, it is used
to compute the logits over the token vocabulary.

\subsection{Bag of Tricks}
\label{ssec:bag_of_tricks}

\subsubsection{Mixing RoBERTa layers}
RoBERTa consists of 24 layers of bidirectional transformer blocks. Given an
input of length $S$, the pretrained RoBERTa encoder will return 25 sequences of
embeddings, $\bG = \{\bg_{\ell i} \in \mathbb{R}^{2048} : i \in \{1, 2,...,
49\}, \ell \in \{ 0, 1,..., 24 \}\}$. This includes the initial
uncontextualized embeddings and the output of each of the 24 layers. Inspired
by Tenney \etal~\cite{Tenney2019BertRT}, who showed that different layers in
BERT represent different steps in the traditional NLP pipeline, we take a
weighted sum across all layers to obtain the article embedding $\bx_{Ti}$:
\begin{IEEEeqnarray*}{lCl}
   \bx_{Ti} &=& \sum_{\ell=0}^{24} \alpha_\ell \, \bg_{\ell i}
\end{IEEEeqnarray*}
where  $\alpha_\ell$ are learnable weights.

\subsubsection{Copying with Multi-headed Attention}

Inspired by pointer-generator networks~\cite{See2017GetTT}, we introduce a
copying mechanism using multi-head attention. We use the final layer output
$\bz_{5t}$ and the article embeddings $\bX_T$ as inputs to the multi-head
attention module. Unlike~\ref{ssection:attn}, we only need to compute the
softmax weights $\lambda_i$ (and not the weighted sum of the values). We
interpret each $\lambda_i$ as the probability of copying $i$th token in the
article.

\subsubsection{Adaptive Softmax}

The decoder BPE vocabulary size is 50265. To make training more efficient, we
use adaptive softmax~\cite{Grave2016EfficientSA} and divide the vocabulary into
three clusters: 5K, 15K, and 25K. We tie the adaptive weights and we share the
decoder input and output embeddings. We use sinusoidal positional encoding
\cite{Vaswani2017AttentionIA} to represent the position of each token.
