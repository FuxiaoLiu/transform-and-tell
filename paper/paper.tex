\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{multirow} % creates cells that span multiple rows in a table
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Writing News Captions with Bags of Attention}

\author{Alasdair Tran, Alexander Matthews, Lexing Xie\\
Australian National University\\
{\tt\small \{alasdair.tran,alex.matthews,lexing.xie\}@anu.edu.au}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   We propose an end-to-end model to generate image captions in news articles.
   By combining the transformer architecture, byte-pair encoding, pretrained
   text and image embeddings, and bags of attentions, our system is able to
   describe an image using specific events and names mentioned in the article.
   Our model achieves state-of-the-art results on the existing GoodNews
   dataset. We also extend the GoodNews dataset to improve the quality of
   articles and study the importance of the image location on the page.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Our main contributions are as follows:

\begin{itemize}
   \item We show that combining article attention with pre-trained
   contextualized word piece embeddings allows our model to generate captions
   containing relevant words that were not seen during training.

   \item We demonstrate that the location of the image within the article is an
   important signal for generating relevant news captions.

   \item We build on the GoodNews dataset to produce a larger and more
   complete news captioning dataset consisting of 445,828 articles from the New
   York Times and their 794K images with captions. In contrast to GoodNews
   our dataset contains the full article text, and the location of the image
   within the page.
\end{itemize}


\section{Related Works}

Recent image captioning systems are based mostly on ``Show, Attend, Tell"
\cite{Xu2015ShowAA}, which uses a simple hard attention over the image. Later
improvements include giving the model the option to not attend to any image
region \cite{Lu2016KnowingWT}, using reinforcement learning to directly
optimize for the CIDEr metric \cite{Rennie2016SelfCriticalST}, and using a
bottom-up approach to propose a region to attend to
\cite{Anderson2017BottomUpAT}. All of these system generate generic captions.

A more challenging task is to generate captions for news images, in which we
need to be able to generate entity names. There are non-neural approaches
\cite{Feng2013AutomaticCG,Tariq2017ACE}. Neural approaches include using the
word2vec representation of the article and and the VGG19 representation of the
image as context to the LSTM generator \cite{Ramisa2016BreakingNewsAA}.

Biten \etal~\cite{Biten2019GoodNews} further refined this approach by
developing a two-stage architecture to handle entity names. In the first stage,
the model generates a template with placeholders such as PERSON and
ORGANIZATION. In the second stage, the model finds the best matched sentence in
the article and replace the placeholder with an entity mentioned in that
sentence. They also created the ``GoodNews" dataset, which contains more
samples than MSCOCO \cite{Lin2014MicrosoftCC} and BreakingNews
\cite{Ramisa2016BreakingNewsAA} datasets.

Zhu \etal \cite{Zhu2018CaptioningTW} used transformers to achieve competitive
results on the MSCOCO dataset. Li \etal \cite{Li2019Boosted} added attention
over image regions and semantic concepts using Faster-RCNN
\cite{Ren2015FasterRCNN}. Both of these works, however, used the MSCOCO dataset
which contains generic captions with no named entities.

Wang \etal \cite{Wang2019Hierarchical} examined different strategies of
combining embeddings from three modalities: concept terms, object regions,
and image patches. These strategies include addition, concatenation, and
multivariate residual modules.

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{An example caption.}
   \label{fig:long}
   \label{fig:onecol}
   \end{figure}

\section{Dataset}

A summary of the original GoodNews dataset and our new dataset is shown in
Table \ref{tab:datasets}. The GoodNews+ dataset is our attempt at recollecting
the original GoodNews dataset. We managed to crawl all the original articles,
but the number of images is slightly less, either due to dead links or
not crawling images unrelated to the main article. GoodNews+ allows us to make the
following observations about the original GoodNews dataset:

\begin{itemize}
   \item On average, half of the article body is missing. This includes
         the first few paragraphs that might contain important information
         about the top caption.
   \item The dataset contains a small number Spanish articles.
   \item Sometimes images from the recommendation sidebar, which are not part
         of the main article, are included in the dataset.
\end{itemize}

This motivates us to recollect data from the New York Times in a more careful
manner, using our own parser. We call this NYTimes+.

\begin{table}[t]
	\caption {Summary of datasets}
	\label{tab:datasets}
	\centering
	\begin{tabular}{llll}
		\toprule
		  & GoodNews  & GoodNews+ &   NYTimes+ \\
		\midrule
      No of articles & 241 808 & 241 808 & 445 828 \\
      No of images   & 462 642 & 440 112 & 794 085 \\
      Article length & 451 & 963 & 974 \\
      Caption length & 18 & 18 & 18 \\
      Start month & Jan 10 & Jan 10 & Mar 05\\
      End month & Jul 18 & Jul 18 & Sep 19 \\
		\bottomrule
	\end{tabular}
\end{table}

In addition to having a more complete coverage, we also collect information
about the image position in each article. As shown in Figure \ref{fig:dist},
about 40\% of the articles have two or more images.

\begin{figure}[h]
   \begin{center}
   \includegraphics[width=0.8\linewidth]{figures/caption_dist.png}
   \end{center}
      \caption{Distribution of images across articles.}
   \label{fig:dist}
   \end{figure}




\section{Model}

Devlin \etal \cite{Devlin2019BERT} introduced BERT, a language representation
model which provides pretrained contextual embeddings for text. We use
RoBERTa \cite{Liu2019RoBERTaAR}, a more carefully trained BERT model, to
embed the article text. There there two key advantages:

\begin{itemize}
   \item Unlike glove \cite{Pennington2014Glove} or word2vec
   \cite{Mikolov2013DistributedRO} where each word has exactly one
   representation, the bidirectional and attention mechanism in transformers
   allow words to have different representation depending on the context.
   \item RoBERTa uses Byte-Pair Encoding (BPE)
   \cite{Sennrich2015NeuralMT,Radford2019LanguageMA} which allows us to
   represent and generate potentially any word that can be written in
   Unicode. The vocabulary size is 50,265.
\end{itemize}

Our decoder is a transformer architecture with dynamic convolutions
\cite{Wu2018PayLA} containing four layers with kernel sizes 3, 7, 15 and 31. We
found that using dynamic convolutions allows the training to converge faster
than using the current state-of-the-art GPT-2 architecture
\cite{Radford2019LanguageMA}. We don't have the computational resource to study
the performance of GPT-2.

To make training more efficient, we use adaptive softmax
\cite{Grave2016EfficientSA} and divide the vocabulary into three clusters: 5K,
15K, and 25K. We tie the adaptive weights and we share the decoder input and
output embeddings. We use sinusoidal positional encoding
\cite{Vaswani2017AttentionIA} to represent the position of each token.

We use ResNeXt-101 \cite{Xie2016AggregatedRT} to obtain an embedding for
the images. For each generation step, we use the Scalar-Dot-Product attention
\cite{Vaswani2017AttentionIA} to attend to the image and to article separate.
The two attended representations are then concatenated and fed as input to
the self-attention layer.

We use MTCNN \cite{Zhang2016JointFD} to detect faces and FaceNet
\cite{Schroff2015FaceNetAU} trained on the VGGFace2 dataset
\cite{Cao2017VGGFace2AD}
to classify the faces.

\begin{figure*}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
   \end{center}
      \caption{Overall architecture of the model.}
   \label{fig:short}
   \end{figure*}

\section{Experiments}

The training pipeline is written in PyTorch \cite{Paszke2017Automatic} using
the AllenNLP framework \cite{Gardner2017AllenNLP}. The RoBERTa model and
dynamic convolution code comes from fairseq \cite{Ott2019Fairseq}. The code
will made public on GitHub.

Table \ref{tab:results-goodnews} shows the results on the original GoodNews
dataset. Table \ref{tab:results-nytimes} shows additional results done on
our NYTimes+ dataset.


\begin{table*}[t]
	\caption {Results on the GoodNews dataset}
	\label{tab:results-goodnews}
	\centering
	\begin{tabular}{llllllll}
		\toprule
		 & BLEU-1  & BLEU-2 & BLEU-3 & BLEU-4 & ROUGE & METEOR & CIDEr\\
      \midrule
      Biten (Avg + CtxIns) \cite{Biten2019GoodNews} & 9.04\% & 3.65\% & 1.71\% & 0.89\% & 12.2\% & 4.37\% & 13.1\% \\
      Biten (Wavg + CtxIns) \cite{Biten2019GoodNews} & 8.10\% & 3.33\% & 1.60\% & 0.85\% & 11.9\% & 4.23\% & 12.8\% \\
      Biten (TBB + CtxIns) \cite{Biten2019GoodNews} & 8.44\% & 3.42\% & 1.62\% & 0.87\% & 12.2\% & 4.30\% & 13.1\% \\
      Biten (Avg + AttIns) \cite{Biten2019GoodNews} & 8.68\% & 3.47\% & 1.58\% & 0.82\% & 12.1\% & 4.23\% & 12.6\% \\
      Biten (Wavg + AttIns) \cite{Biten2019GoodNews} & 7.74\% & 3.15\% & 1.45\% & 0.75\% & 11.9\% & 4.10\% & 12.5\% \\
      Biten (TBB + AttIns) \cite{Biten2019GoodNews} & 8.10\% & 3.26\% & 1.48\% & 0.76\% & 12.2\% & 4.17\% & 12.7\% \\
      \midrule
      LSTM + GloVe & 13.6\% & 6.19\% & 3.13\% & 1.81\% & 13.5\% & 5.26\% & 11.9\% \\
      LSTM + RoBERTa &  &  &  &  &  &  &  \\
      Transformer + GloVe &  &  &  &  &  &  &  \\
      Transformer + RoBERTa & 22.7\% & 13.7\% & 8.93\% & 6.17\% & 21.4\% & 10.3\% & 53.9\% \\
		\bottomrule
	\end{tabular}
\end{table*}


\begin{table*}[t]
   \caption {Proper name metrics on the GoodNews test set. We can't compute rare name statistics
   since train-test split were created using a random shuffle.}
	\label{tab:results-names}
	\centering
	\begin{tabular}{lll}
		\toprule
        & \multicolumn{2}{c}{All names}  \\
        & Recall  & Precision \\
      \midrule
      Biten (Avg + CtxIns) \cite{Biten2019GoodNews} & 12.2\% & 16.5\% \\
      Biten (Wavg + CtxIns) \cite{Biten2019GoodNews} & 12.0\% & 17.1\% \\
      Biten (TBB + CtxIns) \cite{Biten2019GoodNews} & 11.7\% & 17.1\% \\
      Biten (Avg + AttIns) \cite{Biten2019GoodNews} & 11.2\% & 17.9\% \\
      Biten (Wavg + AttIns) \cite{Biten2019GoodNews} & 11.2\% & 19.0\% \\
      Biten (TBB + AttIns) \cite{Biten2019GoodNews} & 11.0\% & 19.2\% \\
      \midrule
      LSTM + GloVe & 10.1\% & 14.9\%  \\
      LSTM + RoBERTa &  &   \\
      Transformer + GloVe & &  \\
      Transformer + RoBERTa & 29.8\% & 34.4\%  \\
		\bottomrule
	\end{tabular}
\end{table*}

\begin{table*}[t]
	\caption {Word statistics on the GoodNews test set}
	\label{tab:results-stats}
	\centering
	\begin{tabular}{lllllll}
		\toprule
        & Caption length  & Unique words \\
      \midrule
      Ground Truths & 18.1 & 17.0 \\
      \midrule
      Biten (Avg + CtxIns) \cite{Biten2019GoodNews}  & 9.9 & 8.8 \\
      Biten (Wavg + CtxIns) \cite{Biten2019GoodNews}  & 9.2 & 8.4 \\
      Biten (TBB + CtxIns) \cite{Biten2019GoodNews}  & 9.2 & 8.4 \\
      Biten (Avg + AttIns) \cite{Biten2019GoodNews}  & 9.7 & 8.4 \\
      Biten (Wavg + AttIns) \cite{Biten2019GoodNews}  & 9.0 & 7.9 \\
      Biten (TBB + AttIns) \cite{Biten2019GoodNews}  & 9.3 & 8.5 \\
      \midrule
      LSTM + GloVe & 13.9 & 11.4 \\
      LSTM + RoBERTa &  &  \\
      Transformer + GloVe & &  \\
      Transformer + RoBERTa & 15.7 & 13.8 \\
		\bottomrule
	\end{tabular}
\end{table*}



\begin{table*}[t]
	\caption {Results on the NYTimes+ test set}
	\label{tab:results-nytimes}
	\centering
	\begin{tabular}{llllllll}
		\toprule
		  & BLEU-1  & BLEU-2 & BLEU-3 & BLEU-4 & ROUGE & METEOR & CIDEr \\
		\midrule
      Transformer (first 512 tokens) & 17.6\% & 10.4\% & 6.68\% & 4.65\% & 19.1\% & 8.49\% & 41.1\% \\
      Transformer (location-aware) & 19.0\% & 11.7\% & 7.76\% & 5.48\% & 21.1\% & 9.51\% & 51.4\% \\
      \\
		\bottomrule
	\end{tabular}
\end{table*}

\begin{table*}[t]
	\caption {Proper name metrics on the NYTimes+ test set}
	\label{tab:results-names}
	\centering
	\begin{tabular}{lllllll}
		\toprule
        & \multicolumn{2}{c}{All names} & \multicolumn{2}{c}{Rare names in caption } & \multicolumn{2}{c}{Rare names in article } \\
        & Recall  & Precision & Recall  & Precision & Recall  & Precision \\
		\midrule
      Transformer (first 512 tokens) & 26.4\% & 31.5\% & 19.8\% & 29.5\% & 17.7\% & 20.9\%  \\
      Transformer (location-aware) & 31.5\% & 36.8\% & 25.3\% & 33.2\% & 23.5\% & 25.3\%   \\
		\bottomrule
	\end{tabular}
\end{table*}

\begin{table*}[t]
	\caption {Word statistics on the NYTimes+ test set}
	\label{tab:results-stats}
	\centering
	\begin{tabular}{lllllll}
		\toprule
        & Caption length  & Unique words \\
      \midrule
      Ground Truths & 18.4 & 17.1 \\
      Transformer (first 512 tokens)  & 13.1 & 11.7 \\
      Transformer (location-aware) & 13.0 & 11.7  \\
		\bottomrule
	\end{tabular}
\end{table*}

\section{Conclusion}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{paper}
}

\end{document}
