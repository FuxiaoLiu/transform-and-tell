\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{multirow} % creates cells that span multiple rows in a table
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{booktabs}

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE
\title{Transform and Tell:\\The News Image Caption Writer That Remembers Rare Names}

\author{Alasdair Tran, Alexander Matthews, Lexing Xie\\
Australian National University\\
{\tt\small \{alasdair.tran,alex.matthews,lexing.xie\}@anu.edu.au}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   We propose an end-to-end model to generate image captions in news articles.
   By combining the transformer architecture, byte-pair encoding, and
   pretrained embeddings from three different modalities (RoBERTa for text,
   ResNet-152 for images, and FaceNet for faces), our system is able to
   describe an image with specific named entities mentioned in the article. Our
   model achieves a CIDEr score of 54 on the GoodNews dataset, significantly
   outperforming the previous state-of-art CIDEr of 13. We also introduce the
   NYTimes+ dataset, the largest news image captioning dataset to date.
   NYTimes+ is an extended version of GoodNews with higher-quality articles and
   metadata that allow us to study the importance of the image location within
   the text. On NYTimes+, we achieve a CIDEr of 51. Pretrained models and
   source code are available from
   \href{https://github.com}{https://anonymized}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Most existing captioning systems can only generate a generic description of an
image \cite{Vinyals2015ShowAT, Fang2015FromCT, Karpathy2015DeepVA,
Rennie2017SelfCriticalST, Lu2017KnowingWT, Anderson2017BottomUpAT}, mainly
because these architectures cannot handle an open vocabulary and early
captioning datasets such as MS COCO \cite{Lin2014MicrosoftCC,
Chen2015MicrosoftCC} and Flickr30k \cite{Young2014FromID} were annotated using
only visual cues present in the image. More recently, datasets such as TIME
\cite{Tariq2017ACE}, BreakingNews \cite{Ramisa2016BreakingNewsAA}, and GoodNews
\cite{Biten2019GoodNews} include real-life captions scraped from online
newspapers. To the best of our knowledge, all existing models that are trained
on these news datasets use template-based methods to insert named entities.

This motivates us to ask whether it is possible to design a fully end-to-end
model for news image captioning. The model would need to 1) attend to
contextual cues from different modalities and 2) generate captions that contain
specific information including rare proper names that have not been seen in the
training data. To answer this, we look at the recent development in
sequence-to-sequence architectures and language modeling.

The most successful family of models in natural language understanding and
generation are the transformer networks, which consist of layers of
self-attention and feed-forward connections stacked on top of each other
\cite{Vaswani2017AttentionIA}. Transformers have consistently been shown to
outperform RNN architectures in language modeling \cite{Radford2019LanguageMA},
story generation \cite{Fan2018HierarchicalNS}, summarization
\cite{Subramanian2019OnEA}, and machine translation \cite{Bojar2018Findings}.

Furthermore, transformers have led to the rise of unsupervised language
representation learning such as BERT \cite{Devlin2019BERT}, XLM
\cite{Lample2019CrosslingualLM}, XLNet \cite{Yang2019XLNetGA}, RoBERTa
\cite{Liu2019RoBERTaAR}, and ALBERT \cite{Lan2019ALBERT}. The contextual word
embeddings provided by these methods have helped establish new state-of-the-art
results on many natural language understanding benchmarks including GLUE
\cite{Wang2019GLUE}, SuperGLUE \cite{Wang2019SuperGLUEAS}, and SQuAD
\cite{Rajpurkar2016SQuAD, Rajpurkar2018KnowWY}, even surpassing the human
baselines in many cases.

\begin{figure}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{0.9\linewidth}{0pt}}
      %\includegraphics[width=0.8\linewidth]{egfigure.eps}
   \end{center}
      \caption{An example caption.}
   \label{fig:long}
   \label{fig:onecol}
\end{figure}

In parallel to the development of these pre-training methods, many novel
techniques have been proposed to improve training convergence and handle more
diverse data inputs. The most significant contribution is the use of byte-pair
encoding \cite{Sennrich2015NeuralMT} to handle an open vocabulary. This is done
by representing a rare word as a sequence of subword units.

The goal of this paper is to make use of these recent developments, along with
their bags of tricks, to carefully design a captioning system most appropriate
for news images. Our main contributions are thus threefold:

\begin{enumerate}
   \item We introduce NYTimes+, the largest news image captioning dataset to
   date, containing 446K articles and 794K images with captions from the New
   York Times spanning 14 years. NYTimes+ is built upon the GoodNews dataset,
   but we write a custom parser to collect higher-quality articles and metadata
   such as the location of an image within the page.

   \item We build a captioning model that combines the power of transformers,
   byte-pair encoding, and attention over three different modalities (text,
   images, and faces). We show that our model achieves state-of-the-art results
   with a significant margin over previous methods, and in particular, it can
   generate names not seen during training without the use of templates.

   \item We provide a detailed model analysis, deconstructing the most
   important modeling components and quantifying the incremental contribution
   that each of them makes not only to the popular metrics such as BLEU, ROUGE,
   METEOR, and CIDEr; but also to other linguistic measures like readability
   scores, caption length, and recall of rare names.
\end{enumerate}


\section{Related Works}

Recent image captioning systems are based mostly on ``Show, Attend, Tell"
\cite{Xu2015ShowAA}, which uses a simple hard attention over the image. Later
improvements include giving the model the option to not attend to any image
region \cite{Lu2017KnowingWT}, using reinforcement learning to directly
optimize for the CIDEr metric \cite{Rennie2017SelfCriticalST}, and using a
bottom-up approach to propose a region to attend to
\cite{Anderson2017BottomUpAT}. All of these systems generate generic captions.

A more challenging task is to generate captions for news images, in which we
need to be able to generate entity names. There are non-neural approaches
\cite{Feng2013AutomaticCG,Tariq2017ACE}. Neural approaches include using the
word2vec representation of the article and and the VGG19 representation of the
image as context to the LSTM generator \cite{Ramisa2016BreakingNewsAA}.

Biten \etal~\cite{Biten2019GoodNews} further refined this approach by
developing a two-stage architecture to handle entity names. In the first stage,
the model generates a template with placeholders such as PERSON and
ORGANIZATION. In the second stage, the model finds the best matched sentence in
the article and replace the placeholder with an entity mentioned in that
sentence. They also created the ``GoodNews" dataset, which contains more
samples than MSCOCO and BreakingNews
\cite{Ramisa2016BreakingNewsAA} datasets.

Zhu \etal \cite{Zhu2018CaptioningTW} used transformers to achieve competitive
results on the MSCOCO dataset. Li \etal \cite{Li2019Boosted} added attention
over image regions and semantic concepts using Faster-RCNN
\cite{Ren2015FasterRCNN}. Both of these works, however, used the MSCOCO dataset
which contains generic captions with no named entities.

Wang \etal \cite{Wang2019Hierarchical} examined different strategies of
combining embeddings from three modalities: concept terms, object regions,
and image patches. These strategies include addition, concatenation, and
multivariate residual modules.



\section{Datasets}

\begin{table}[t]
	\caption {Summary of datasets}
	\label{tab:datasets}
	\centering
	\begin{tabular}{llll}
		\toprule
		  & GoodNews  & GoodNews+ &   NYTimes+ \\
		\midrule
      No. of articles & 241 808 & 241 808 & 445 828 \\
      No. of images   & 462 642 & 440 112 & 794 085 \\
      Article length & 451 & 963 & 974 \\
      Caption length & 18 & 18 & 18 \\
      Start month & Jan 10 & Jan 10 & Mar 05\\
      End month & Jul 18 & Jul 18 & Sep 19 \\
		\bottomrule
	\end{tabular}
\end{table}

\subsection{GoodNews and GoodNews+}

As a starting point, we use the GoodNews dataset, previously the largest
dataset on news image captioning. As part of the quality check, we also
reconstruct the same dataset on our own. We use the New York Times API and
write a custom parser to recollect and extract all the articles and images in
in the same period as GoodNews from the original source. We call this
GoodNews+. We managed to crawl all the original articles, but the number of
images is slightly less, either due to dead links or not crawling images
unrelated to the main article. GoodNews+ allows us to make the following
observations about the original GoodNews dataset:

\begin{itemize}
   \item The average length of an article in GoodNews is 451 words, while it is
         963 words in GoodNews+. Thus half of the article body is missing in
         GoodNews. This includes the first few paragraphs that might contain
         important information about the top caption. Looking through the
         publicly released code, we found that this is because the original
         parser used by \cite{Biten2019GoodNews} fails to recognize certain
         HTML tags used in New York Times web pages.
   \item GoodNews contains X Spanish articles, adding unwanted noise to the
         training.
   \item GoodNews sometimes include images from the recommendation sidebar,
         which are not part of the main article.
\end{itemize}

Despite the above limitation, we will use the original GoodNews dataset in our
experiments, along with the original train-validation-test split to make
comparison to previous works easier.

\subsection{NYTimes+}

At the same time, we also construct a new dataset, which we call NYTimes+.
NYTimes+ exhibits several advantages over GoodNews:

\begin{itemize}
   \item By increasing the collection period to the last 14 years (March 2005
   -- September 2019), NYTimes+ contains 80\% more articles and images, thus
   becoming the largest news captioning dataset.
   \item NYTimes+ contains only English articles.
   \item We are careful to include only images that are part of the main
   article.
   \item In addition to having a more complete coverage, we also collect
   information about the image position in each article. As shown in Figure
   \ref{fig:dist}, about 40\% of the articles have two or more images.
\end{itemize}

A summary of the datasets is shown in Table \ref{tab:datasets}.


\begin{figure}[t]
   \begin{center}
   \includegraphics[width=0.8\linewidth]{figures/caption_dist.png}
   \end{center}
      \caption{Distribution of images across articles.}
   \label{fig:dist}
   \end{figure}

\section{Model}

Devlin \etal \cite{Devlin2019BERT} introduced BERT, a language representation
model which provides pretrained contextual embeddings for text. We use
RoBERTa \cite{Liu2019RoBERTaAR}, a more carefully trained BERT model, to
embed the article text. There there two key advantages:

\begin{itemize}
   \item Unlike glove \cite{Pennington2014Glove} or word2vec
   \cite{Mikolov2013DistributedRO} where each word has exactly one
   representation, the bidirectional and attention mechanism in transformers
   allow words to have different representation depending on the context.
   \item RoBERTa uses Byte-Pair Encoding (BPE)
   \cite{Sennrich2015NeuralMT,Radford2019LanguageMA} which allows us to
   represent and generate potentially any word that can be written in
   Unicode. The vocabulary size is 50,265.
\end{itemize}

Our decoder is a transformer architecture with dynamic convolutions
\cite{Wu2018PayLA} containing four layers with kernel sizes 3, 7, 15 and 31. We
found that using dynamic convolutions allows the training to converge faster
than using the current state-of-the-art GPT-2 architecture
\cite{Radford2019LanguageMA}. We don't have the computational resource to study
the performance of GPT-2.

To make training more efficient, we use adaptive softmax
\cite{Grave2016EfficientSA} and divide the vocabulary into three clusters: 5K,
15K, and 25K. We tie the adaptive weights and we share the decoder input and
output embeddings. We use sinusoidal positional encoding
\cite{Vaswani2017AttentionIA} to represent the position of each token.

We use ResNeXt-101 \cite{Xie2017AggregatedRT} to obtain an embedding for
the images. For each generation step, we use the Scalar-Dot-Product attention
\cite{Vaswani2017AttentionIA} to attend to the image and to article separate.
The two attended representations are then concatenated and fed as input to
the self-attention layer.

We use MTCNN \cite{Zhang2016JointFD} to detect faces and FaceNet
\cite{Schroff2015FaceNetAU} trained on the VGGFace2 dataset
\cite{Cao2017VGGFace2AD}
to classify the faces.

\begin{figure*}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
   \end{center}
      \caption{Overall architecture of the model.}
   \label{fig:short}
   \end{figure*}

\section{Experiments}

The training pipeline is written in PyTorch \cite{Paszke2017Automatic} using
the AllenNLP framework \cite{Gardner2017AllenNLP}. The RoBERTa model and
dynamic convolution code comes from fairseq \cite{Ott2019Fairseq}. The code
will made public on GitHub.

Table \ref{tab:results-goodnews} shows the results on the original GoodNews
dataset. Table \ref{tab:results-nytimes} shows additional results done on
our NYTimes+ dataset.


\begin{table*}[t]
	\caption {Results on the GoodNews dataset}
	\label{tab:results-goodnews}
	\centering
	\begin{tabular}{llllllll}
		\toprule
		 & BLEU-1  & BLEU-2 & BLEU-3 & BLEU-4 & ROUGE & METEOR & CIDEr\\
      \midrule
      Biten (Avg + CtxIns) \cite{Biten2019GoodNews} & 9.04 & 3.65 & 1.71 & 0.89 & 12.2 & 4.37 & 13.1 \\
      Biten (Wavg + CtxIns) \cite{Biten2019GoodNews} & 8.10 & 3.33 & 1.60 & 0.85 & 11.9 & 4.23 & 12.8 \\
      Biten (TBB + CtxIns) \cite{Biten2019GoodNews} & 8.44 & 3.42 & 1.62 & 0.87 & 12.2 & 4.30 & 13.1 \\
      Biten (Avg + AttIns) \cite{Biten2019GoodNews} & 8.68 & 3.47 & 1.58 & 0.82 & 12.1 & 4.23 & 12.6 \\
      Biten (Wavg + AttIns) \cite{Biten2019GoodNews} & 7.74 & 3.15 & 1.45 & 0.75 & 11.9 & 4.10 & 12.5 \\
      Biten (TBB + AttIns) \cite{Biten2019GoodNews} & 8.10 & 3.26 & 1.48 & 0.76 & 12.2 & 4.17 & 12.7 \\
      \midrule
      GloVe + LSTM & 13.6 & 6.19 & 3.13 & 1.81 & 13.5 & 5.26 & 11.9 \\
      GloVe + Transformer & 17.2 & 8.68 & 4.81 & 2.91 & 16.4 & 7.13 & 22.2 \\
      RoBERTa + LSTM & 18.7  & 10.3 & 6.09 & 3.89  & 18.0  & 8.19 & 34.8  \\
      RoBERTa + Transformer & 22.7 & 13.7 & 8.93 & 6.17 & 21.4 & 10.3 & 53.9 \\
		\bottomrule
	\end{tabular}
\end{table*}


\begin{table*}[t]
   \caption {Proper name metrics on the GoodNews test set. We can't compute rare name statistics
   since train-test split were created using a random shuffle.}
	\label{tab:results-names}
	\centering
	\begin{tabular}{lll}
		\toprule
        & \multicolumn{2}{c}{All names}  \\
        & Recall  & Precision \\
      \midrule
      Biten (Avg + CtxIns) \cite{Biten2019GoodNews} & 12.2\% & 16.5\% \\
      Biten (Wavg + CtxIns) \cite{Biten2019GoodNews} & 12.0\% & 17.1\% \\
      Biten (TBB + CtxIns) \cite{Biten2019GoodNews} & 11.7\% & 17.1\% \\
      Biten (Avg + AttIns) \cite{Biten2019GoodNews} & 11.2\% & 17.9\% \\
      Biten (Wavg + AttIns) \cite{Biten2019GoodNews} & 11.2\% & 19.0\% \\
      Biten (TBB + AttIns) \cite{Biten2019GoodNews} & 11.0\% & 19.2\% \\
      \midrule
      GloVe + LSTM & 10.1\% & 14.9\%  \\
      GloVe + Transformer & 16.6\% & 21.7\% \\
      RoBERTa + LSTM & 23.4\% & 28.8\%  \\
      RoBERTa + Transformer & 29.8\% & 34.4\%  \\
		\bottomrule
	\end{tabular}
\end{table*}

\begin{table*}[t]
	\caption {Word statistics on the GoodNews test set}
	\label{tab:results-stats}
	\centering
	\begin{tabular}{lllllll}
		\toprule
        & Caption length  & Unique words \\
      \midrule
      Ground Truths & 18.1 & 17.0 \\
      \midrule
      Biten (Avg + CtxIns) \cite{Biten2019GoodNews}  & 9.9 & 8.8 \\
      Biten (Wavg + CtxIns) \cite{Biten2019GoodNews}  & 9.2 & 8.4 \\
      Biten (TBB + CtxIns) \cite{Biten2019GoodNews}  & 9.2 & 8.4 \\
      Biten (Avg + AttIns) \cite{Biten2019GoodNews}  & 9.7 & 8.4 \\
      Biten (Wavg + AttIns) \cite{Biten2019GoodNews}  & 9.0 & 7.9 \\
      Biten (TBB + AttIns) \cite{Biten2019GoodNews}  & 9.3 & 8.5 \\
      \midrule
      GloVe + LSTM & 13.9 & 11.4 \\
      GloVe + Transformer & 14.9 & 12.9 \\
      RoBERTa + LSTM & 14.7 & 12.7  \\
      RoBERTa + Transformer & 15.7 & 13.8 \\
		\bottomrule
	\end{tabular}
\end{table*}



\begin{table*}[t]
	\caption {Results on the NYTimes+ test set}
	\label{tab:results-nytimes}
	\centering
	\begin{tabular}{llllllll}
		\toprule
		  & BLEU-1  & BLEU-2 & BLEU-3 & BLEU-4 & ROUGE & METEOR & CIDEr \\
      \midrule
      GloVe + LSTM & 14.0 & 6.09 & 2.98 & 1.67 & 13.0 & 5.15 & 9.96 \\
      GloVe + Transformer & 17.0 & 8.42 & 4.63 & 2.79 & 16.1 & 6.99 & 20.6 \\
      RoBERTa + LSTM & 18.6 & 10.1 & 6.03 & 3.88 & 17.1 & 8.00 & 29.2 \\
      Transformer (first 512 tokens) & 17.6 & 10.4 & 6.68 & 4.65 & 19.1 & 8.49 & 41.1 \\
      Transformer (location-aware) & 19.0 & 11.7 & 7.76 & 5.48 & 21.1 & 9.51 & 51.4 \\
      \\
		\bottomrule
	\end{tabular}
\end{table*}

\begin{table*}[t]
	\caption {Proper name metrics on the NYTimes+ test set}
	\label{tab:results-names}
	\centering
	\begin{tabular}{lllllll}
		\toprule
        & \multicolumn{2}{c}{All names} & \multicolumn{2}{c}{Rare names in caption } & \multicolumn{2}{c}{Rare names in article } \\
        & Recall  & Precision & Recall  & Precision & Recall  & Precision \\
      \midrule
      GloVe + LSTM & 10.5\% & 13.1\% & 0\% & 0\% & 0\% & 0\%  \\
      GloVe + Transformer & 18.3\% & 21.7\% & 0\% & 0\% & 0\% & 0\%  \\
      RoBERTa + LSTM & 25.1\% & 26.7\% & 14.8\% & 14.0\% & 12.5\% & 7.98\%  \\
      Transformer (first 512 tokens) & 26.4\% & 31.5\% & 19.8\% & 29.5\% & 17.7\% & 20.9\%  \\
      Transformer (location-aware) & 31.5\% & 36.8\% & 25.3\% & 33.2\% & 23.5\% & 25.3\%   \\
		\bottomrule
	\end{tabular}
\end{table*}

\begin{table*}[t]
	\caption {Word statistics on the NYTimes+ test set}
	\label{tab:results-stats}
	\centering
	\begin{tabular}{lllllll}
		\toprule
        & Caption length  & Unique words \\
      \midrule
      Ground Truths & 18.4 & 17.1 \\
      GloVe + LSTM  & 15.3 & 12.3 \\
      GloVe + Transformer  & 15.1 & 12.9 \\
      RoBERTa + LSTM  & 15.6 & 13.4 \\
      Transformer (first 512 tokens)  & 13.1 & 11.7 \\
      Transformer (location-aware) & 13.0 & 11.7  \\
		\bottomrule
	\end{tabular}
\end{table*}


\begin{table*}[t]
	\caption {Model complexity: Number of trainable parameters}
	\label{tab:models}
	\centering
	\begin{tabular}{lllllll}
		\toprule
        & Parameters \\
      \midrule
      GloVe + LSTM & 153M \\
      GloVe + Transformer & 148M \\
      RoBERTa + LSTM & 139M \\
      RoBERTa + Transformer & 154M \\
        + faces & 171M \\
        + names & 175M \\
        + faces + names & 192M \\
		\bottomrule
	\end{tabular}
\end{table*}

\section{Conclusion}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{paper}
}

\end{document}
