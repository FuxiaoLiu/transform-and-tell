\documentclass[10pt,twocolumn,letterpaper]{article}

\usepackage{cvpr}
\usepackage{times}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{multirow} % creates cells that span multiple rows in a table
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{bm} % bold fonts for math symbols
\usepackage{booktabs}
\usepackage[export]{adjustbox} % adds a frame around figures
\usepackage{IEEEtrantools} % IEEEeqnarray math equation environment
\usepackage{multirow} % merge multiple rows
\usepackage{tabularx} % for 'tabularx' environment
\usepackage{caption} % controls spacing between caption and table

% Include other packages here, before hyperref.

% If you comment hyperref and then uncomment it, you should delete
% egpaper.aux before re-running latex.  (Or just hit 'q' on the first latex
% run, let it finish, and you should be clear).
\usepackage[pagebackref=true,breaklinks=true,letterpaper=true,colorlinks,bookmarks=false]{hyperref}

% \cvprfinalcopy % *** Uncomment this line for the final submission

\def\cvprPaperID{****} % *** Enter the CVPR Paper ID here
\def\httilde{\mbox{\tt\raisebox{-.5ex}{\symbol{126}}}}

% Pages are numbered in submission mode, and unnumbered in camera-ready
\ifcvprfinal\pagestyle{empty}\fi
\begin{document}

%%%%%%%%% TITLE - suggestions and candidates
\title{Transform and Tell:\\The News Image Caption Writer That Remembers Rare Names}
\title{Transforming Entities for News Image Captions}

\author{Alasdair Tran, Alexander P. Matthews, Lexing Xie\\
Australian National University\\
{\tt\small \{alasdair.tran,alex.matthews,lexing.xie\}@anu.edu.au}
% For a paper whose authors are all at the same institution,
% omit the following lines up until the closing ``}''.
% Additional authors and addresses can be added with ``\and'',
% just like the second author.
% To save space, use either the email address or home page, not both
}

\maketitle
%\thispagestyle{empty}

%%%%%%%%% ABSTRACT
\begin{abstract}
   We propose an end-to-end model to generate image captions in news articles.
   By combining the transformer architecture, byte-pair encoding, and
   pretrained embeddings from three different modalities (RoBERTa for text,
   ResNet-152 for images, and FaceNet for faces), our system is able to
   describe an image with specific named entities mentioned in the article. Our
   model achieves a CIDEr score of 54 on the GoodNews dataset, significantly
   outperforming the previous state-of-art CIDEr of 13. We also introduce the
   NYTimes800k dataset, the largest news image captioning dataset to date.
   NYTimes800k is an extended version of GoodNews with higher-quality articles and
   metadata that allow us to study the importance of the image location within
   the text. On NYTimes800k, we achieve a CIDEr of 55. Pretrained models and
   source code are available from
   \href{https://github.com}{https://github.com/anonymized-link}.
\end{abstract}

%%%%%%%%% BODY TEXT
\section{Introduction}

Most existing captioning systems can only generate a generic description of an
image \cite{Donahue2015LongTR, Vinyals2015ShowAT, Fang2015FromCT,
Karpathy2015DeepVA, Rennie2017SelfCriticalST, Lu2017KnowingWT,
Anderson2017BottomUpAT, Cornia2019ShowCT}, mainly because these architectures
cannot handle an open vocabulary and early captioning datasets such as MS COCO
\cite{Lin2014MicrosoftCC, Chen2015MicrosoftCC} and Flickr30k
\cite{Young2014FromID} were annotated using only visual cues present in the
image. More recently, datasets such as TIME \cite{Tariq2017ACE}, BreakingNews
\cite{Ramisa2016BreakingNewsAA}, and GoodNews \cite{Biten2019GoodNews} include
real-life captions written by professional journalists. However, existing
models that are trained on these news datasets still need to rely on either
extractive methods or templates to insert named entities.

This motivates us to ask whether it is possible to design a fully end-to-end
model for news image captioning. The model would need to 1) attend to
contextual cues from different modalities and 2) generate captions that contain
specific information including rare proper names that have not been seen in the
training data. To answer this, we look at the recent development in
sequence-to-sequence architectures and language representation learning.

The most successful family of models in the field of natural language
processing (NLP) are the transformer networks, which consist of layers of
self-attention and feed-forward connections stacked on top of each other
\cite{Vaswani2017AttentionIA}. Transformers have consistently been shown to
outperform RNN architectures in language modeling \cite{Radford2019LanguageMA},
story generation \cite{Fan2018HierarchicalNS}, summarization
\cite{Subramanian2019OnEA}, and machine translation \cite{Bojar2018Findings}.
There are theoretical justifications that transformers could be universal
approximations of sequence-to-sequence functions \cite{Anonymous2020AreTU}.

Furthermore, transformers have led to the rise of unsupervised language
representation learning such as BERT \cite{Devlin2019BERT}, XLM
\cite{Lample2019CrosslingualLM}, XLNet \cite{Yang2019XLNetGA}, RoBERTa
\cite{Liu2019RoBERTaAR}, and ALBERT \cite{Lan2019ALBERT}. The contextual word
embeddings provided by these methods have helped establish new state-of-the-art
results on many natural language understanding benchmarks including GLUE
\cite{Wang2019GLUE}, SuperGLUE \cite{Wang2019SuperGLUEAS}, and SQuAD
\cite{Rajpurkar2016SQuAD, Rajpurkar2018KnowWY}, even surpassing the human
baselines in many cases.

\begin{figure}[t]
   \begin{center}
      \includegraphics[width=0.99\linewidth, frame]{figures/figure_1.pdf}
   \end{center}
      \caption{Our transformer model attends to embeddings from three different
      domains (image patches, faces, and article text). Using byte-pair
      encoding, the model can then directly produce a caption containing
      specific named entities without the use of templates.}
   \label{fig:long}
   \label{fig:onecol}
\end{figure}

In parallel to the development of these pre-training methods, many novel
techniques have been proposed to improve training convergence and handle more
diverse data inputs. The most significant contribution is the use of byte-pair
encoding (BPE) \cite{Sennrich2015NeuralMT} to represent a rare word as a
sequence of subword units, thus giving models the ability to handle an open
vocabulary.

The goal of this paper is to make use of these recent developments, along with
their bags of tricks, to carefully design a captioning system most appropriate
for news images. Our main contributions are thus threefold:

\begin{enumerate}
   \item We introduce NYTimes800k, the largest news image captioning dataset to
   date, containing 446K articles and 794K images with captions from The New
   York Times spanning 14 years. NYTimes800k is built upon the GoodNews dataset,
   but we write a custom parser to collect higher-quality articles and metadata
   such as the location of an image within the page.

   \item We build a captioning model that combines the power of transformers,
   byte-pair encoding, and attention over three different modalities (text,
   images, and faces). We show that our model achieves state-of-the-art results
   with a significant margin over previous methods, and in particular, it can
   generate names not seen during training without the use of templates.

   \item We provide a detailed model analysis, deconstructing the most
   important modeling components and quantifying the incremental contribution
   that each of them makes not only to the usual metrics such as BLEU, ROUGE,
   METEOR, and CIDEr; but also to other linguistic measures like readability
   scores, caption length, and recall of rare names.
\end{enumerate}


\section{Related Works}

Early image captioning systems mainly used an RNN architecture with a closed
vocabulary \cite{Karpathy2015DeepVA, Donahue2015LongTR, Vinyals2015ShowAT}.
Attention over image patches was later introduced in ``Show, Attend and Tell"
\cite{Xu2015ShowAA}, in which the attention weights are obtained by feeding the
image embeddings and the previous hidden state of the RNN through a multilayer
perception. Later improvements include giving the model the option to not
attend to any image region \cite{Lu2017KnowingWT}, using reinforcement learning
to directly optimize for the CIDEr metric \cite{Rennie2017SelfCriticalST,
Gao2019DeliberateAN}, and using a bottom-up approach to propose a region to
attend to \cite{Anderson2017BottomUpAT}. All of these systems can only generate
generic captions with a limited vocabulary.

A more challenging task is to produce captions for news images, in which we
need to be able to generate entity names. Early non-neural approaches include
extractive methods that use n-gram models to combine existing phrases
\cite{Feng2013AutomaticCG} or simply retrieving the most representative
sentence \cite{Tariq2017ACE} in the article. Ramisa \etal
\cite{Ramisa2016BreakingNewsAA} concatenated the word2vec representation of the
article and and the VGG19 representation of the image, and feed them as inputs
to an LSTM generator. However the generator still cannot produce names not
seen in training.

To overcome the limitation of a fixed-size vocabulary, template-based methods
have been used to insert named entities. This involves first generating a
template sentence with placeholders, e.g. ``PERSON speaks at BUILDING in
DATE.'' Afterwards, a selection algorithm is used to pick the best candidate
for each placeholder. Lu \etal \cite{Lu2018EntityAI} built a knowledge graph
for each combination of entities and select the most likely combination.
Meanwhile Biten \etal~\cite{Biten2019GoodNews}, whose GoodNews dataset we will
benchmark against, picked the sentence with the highest cosine similarity with
the template and then found the first entity that matches the type of each
placeholder for insertion. Our proposed model differs from
\cite{Lu2018EntityAI} and \cite{Biten2019GoodNews} in that we are able to
generate a caption with named entities directly without using any intermediate
template.

Transformers are still scarcely used in image captioning. They have been shown
to yield competitive results in generating generic MS COCO captions
\cite{Zhu2018CaptioningTW, Li2019Boosted}. Zhao \etal
\cite{Zhao2019InformativeIC} have gone further and trained transformers to
produce named entities in the Conceptual Captions dataset
\cite{Sharma2018ConceptualCA}. However Conceptual Captions have no additional
context apart from the image itself, and the authors used web-entity labels,
extracted using Google Cloud Vision API, as inputs to the model. In our work,
we are more ambitious in that we do not explicitly give the model a list of
entities that should appear in the caption. Instead the model has to determine
on its own which entities to generate by scanning through and attending to the
article.

BPE offers an elegant solution to handling an open vocabulary. To date the only
image captioning work that uses BPE is \cite{Sharma2018ConceptualCA}, but in
their data preprocessing step, they explicitly removed rare named entities from
the captions. We attempt to fill this gap and in particular examine how much
better BPE can generate rare names compared to template-based methods.

In addition to attending to image patches, some captioning models also attend
to object regions \cite{Wang2019Hierarchical} and visual concepts
\cite{You2016ImageCW,Li2019Boosted,Wang2019Hierarchical}, both of which are
derived from the image itself. When attending to more than one modality, there
are various strategies on how to combine embeddings such as addition,
concatenation, and using multivariate residual modules (MRMs)
\cite{Kim2016MultimodalRL}. In our models, we use a simple concatenation since
more complex strategies such as MRMs have shown to yield only minor improvement
\cite{Wang2019Hierarchical}.



\section{Datasets}

\subsection{GoodNews}

As a starting point, we use the GoodNews dataset, previously the largest
dataset on news image captioning \cite{Biten2019GoodNews}. Each sample in the
dataset is a triplet containing an article, an image, and a caption. Since only
the article text and captions were publicly released, we had to crawl the
images ourselves. Out of the 466K image URLs provided by
\cite{Biten2019GoodNews}, we were able to crawl 463K images, or 99.2\% of the
original dataset. The remaining are broken links. As part of the quality check,
we also wrote a custom parser to re-extract the articles in the same period
from the original web pages, with the help of The New York Times
API\footnote{\href{https://developer.nytimes.com/apis}{https://developer.nytimes.com/apis}}.

% \begin{table}[t]
% 	\caption {Summary of datasets}
% 	\label{tab:datasets}
% 	\centering
% 	\begin{tabular}{llll}
% 		\toprule
% 		  & GoodNews  & GoodNews+ &   NYTimes800k \\
% 		\midrule
%       No. of articles & 241 808 & 241 808 & 445 828 \\
%       No. of images   & 462 642 & 440 112 & 794 085 \\
%       Article length & 451 & 963 & 974 \\
%       Caption length & 18 & 18 & 18 \\
%       Start month & Jan 10 & Jan 10 & Mar 05\\
%       End month & Jul 18 & Jul 18 & Sep 19 \\
% 		\bottomrule
% 	\end{tabular}
% \end{table}


\begin{table}[t]
	\caption {Summary of news captioning datasets}
	\label{tab:datasets}
	\centering
	\begin{tabularx}{\linewidth}{lXX}
		\toprule
		  & GoodNews  &   NYTimes800k \\
		\midrule
      Number of articles & 257 033 & 445 819 \\
      Number of images   & 462 642 & 794 044 \\
      Average article length & 451 & 974 \\
      Average caption length & 18 & 18 \\
      Collection start month & Jan 10 & Mar 05\\
      Collection end month & Mar 18 & Sep 19 \\
      \midrule
      \% of words that are \\
      \quad -- nouns & 16\% & 16\% \\
      \quad -- pronouns & 1\% & 1\% \\
      \quad -- proper nouns & 23\% & 22\% \\
      \quad -- verbs & 9\% & 9\%  \\
      \quad -- adjectives & 4\% & 4\% \\
      \quad -- named entities & 27\% & 26\% \\
      \quad -- personal names & 9\% & 9\% \\
      \midrule
      \% of captions with \\
      \quad -- named entities & 97\% & 96\% \\
      \quad -- personal names & 68\% & 68\% \\
		\bottomrule
	\end{tabularx}
\end{table}

From the recollection, we observe that the average length of an article in
GoodNews is 451 words, while it is 963 in our recollected dataset. Thus half of
the article body is missing in GoodNews. This includes the first few paragraphs
that might contain important information about the top caption. Looking through
the publicly released code, we found that this is because the original parser
used by \cite{Biten2019GoodNews} was designed to extract articles from a
generic online news source and it fails to recognize certain HTML tags used
specifically in New York Times web pages.

In addition, we also found some noisy samples in GoodNews. This includes a
small number of non-English articles, and captioned images from the
recommendation sidebar which are not related to the main article.

Despite the above limitations, we still use the original GoodNews dataset and
train-validation-test split in our experiments, in order to make comparison to
previous works easier. Using the original split, we have 421K training, 18K
validation, and 23K test captions. Given that the random split was done at the
caption level, it is possible for a training and test caption to share the same
article text.


\subsection{NYTimes800k}

At the same time, we also constructed a bigger and higher-quality dataset,
which we call NYTimes800k. A comparison between GoodNews and NYTimes800k is
summarized in Table \ref{tab:datasets}. NYTimes800k exhibits several
advantages:

\begin{figure}[t]
   \begin{center}
   \includegraphics[width=0.99\linewidth]{figures/figure_2_entities.pdf}
   \end{center}
      \caption{Entity distribution in NYTimes800k training captions. The four
               most common entity types are personal names, geopolitical
               entities, organizations, and dates.}
   \label{fig:entities}
\end{figure}

\begin{figure}[t]
   \begin{center}
   \includegraphics[width=0.99\linewidth]{figures/figure_3_faces.pdf}
   \end{center}
      \caption{Co-occurrence of faces and personal names in NYTimes800k
               training data. The blue bars count how many images containing a
               certain number of faces. The orange bars count how many captions
               containing a certain number of personal names.}
   \label{fig:faces}
\end{figure}

\begin{figure*}[t]
   \begin{center}
   \fbox{\rule{0pt}{2in} \rule{.9\linewidth}{0pt}}
   \end{center}
      \caption{Overall architecture of the model.}
   \label{fig:short}
\end{figure*}


\begin{itemize}
   \item By increasing the collection period to the last 14 years (March 2005
   -- September 2019), NYTimes800k contains 80\% more articles and images, thus
   becoming the largest news image captioning dataset.
   \item Using our custom parser, articles in NYTimes800k contain the full text
   with no missing paragraphs.
   \item NYTimes800k contains only English articles.
   \item We are careful to include only images that are part of the main
   article.
   \item Unlike GoodNews, we also collect information about where an image is
   located in the corresponding article. Most news articles have one image at
   the top that relates to the key topic. However 39\% of the articles have at
   least one more image somewhere in the middle of text. The image placement
   and hence the text surrounding the image could be important information for
   captioning.
\end{itemize}

Entities play an important role in the dataset, with 97\% of captions
containing at least one named entity. As shown in Figure \ref{fig:entities},
the most popular entity type are personal names, comprising a third of all
named entities. Furthermore, 71\% of training images contain at least one face
and 68\% of training captions mention at least one personal name. Figure
\ref{fig:faces} provides a further breakdown of the co-occurrence of faces and
personal names. One important observation is that the majority of captions
contain at most four names.


\begin{table}[t]
	\caption {NYTimes800k training, validation, and test splits}
	\label{tab:splits}
	\centering
	\begin{tabularx}{\linewidth}{lXXX}
		\toprule
		  & Training  &   Validation & Test \\
		\midrule
      Number of articles & 434 272 & 3 052 & 8 495 \\
      Number of images  & 764 049 & 7 852 & 22 143 \\
      Start month & Mar 15 & May 19 & Jun 19 \\
      End month & Apr 19 & May 19 & Aug 19 \\
		\bottomrule
	\end{tabularx}
\end{table}

We split the training, validation, and test sets according to time, as shown in
Table \ref{tab:splits}. For example, the test set consists of all captions and
articles in the final three months of the collection period, from June to
August 2019. This has two advantages over the random split used in GoodNews.
Firstly, it prevents captions in the training and test sets from sharing the
same context article, thus making it better to see how well the model can
generalize. Secondly, due of the shift in the coverage of news over time, there
will be events and people in the test data that have never covered by the news
before. In particular, out of the 100K proper nouns in the test captions, 4\%
never appear in any training captions. Half of these also never appear in any
training article. Thus splitting by time allows us study how well the model can
generate rare names.



\begin{table*}[t]
   \caption {BLEU, ROUGE, METEOR, and CIDEr metrics on GoodNews and
             NYTimes800k.}

	\label{tab:results}
	\centering
	\begin{tabularx}{\textwidth}{llXXXXXXX}
		\toprule
		 & & BLEU-1  & BLEU-2 & BLEU-3 & BLEU-4 & ROUGE & METEOR & CIDEr\\
      \midrule
      \multirow{8}{*}{\rotatebox[origin=c]{90}{GoodNews}}
      & Biten (Avg + CtxIns) \cite{Biten2019GoodNews} & 9.04 & 3.66 & 1.71 & 0.89 & 12.2 & 4.37 & 13.1 \\
      % Biten (Wavg + CtxIns) \cite{Biten2019GoodNews} & 8.10 & 3.33 & 1.60 & 0.85 & 11.9 & 4.23 & 12.8 \\
      % Biten (TBB + CtxIns) \cite{Biten2019GoodNews} & 8.44 & 3.42 & 1.62 & 0.87 & 12.2 & 4.30 & 13.1 \\
      % Biten (Avg + AttIns) \cite{Biten2019GoodNews} & 8.68 & 3.47 & 1.58 & 0.82 & 12.1 & 4.23 & 12.6 \\
      % Biten (Wavg + AttIns) \cite{Biten2019GoodNews} & 7.74 & 3.15 & 1.45 & 0.75 & 11.9 & 4.10 & 12.5 \\
      & Biten (TBB + AttIns) \cite{Biten2019GoodNews} & 8.10 & 3.26 & 1.48 & 0.76 & 12.2 & 4.17 & 12.7 \\
      \cmidrule{2-9}
      % & GloVe + LSTM & 13.6 & 6.19 & 3.13 & 1.81 & 13.5 & 5.26 & 11.9 \\
      % & GloVe + Transformer & 17.2 & 8.68 & 4.81 & 2.91 & 16.4 & 7.13 & 22.2 \\
      % & RoBERTa + LSTM & 18.7  & 10.3 & 6.09 & 3.89  & 18.0  & 8.19 & 34.8  \\
      & GloVe + LSTM &  &  &  &  &  &  &  \\
      & GloVe + Transformer &  &  &  &  &  &  &  \\
      & RoBERTa + LSTM &  &  &  &  &  &  &   \\
      & RoBERTa + Transformer & 22.2 & 13.4 & 8.68 & 5.99 & 21.2 & 10.1 & 52.9 \\ % Final
      & \quad + faces & 22.4 & 13.6 & 8.84 & 6.10 & 21.3 & 10.3 & 53.9 \\ % Final
      & \quad\quad + copying \\
      \midrule
      \multirow{6}{*}{\rotatebox[origin=c]{90}{NYTimes800k}}
      & GloVe + LSTM & 13.4 & 6.00 & 3.05 & 1.76 & 13.2 & 5.36 & 12.2 \\ % Final
      & GloVe + Transformer & 17.0 & 8.42 & 4.63 & 2.79 & 16.1 & 6.99 & 20.6 \\ % Final
      & RoBERTa + LSTM & 18.0 & 9.88 & 5.97 & 3.91 & 17.1 & 7.96 & 30.8 \\ % Final
      & RoBERTa + Transformer & 20.7 & 12.4 & 8.14 & 5.73 & 19.8 & 9.54 & 44.1 \\ % Final
      & \quad + location-aware & 21.7 & 13.3 & 8.84 & 6.25 & 21.3 & 10.3 & 52.4 \\ % Final
      & \quad\quad + faces & 22.1 & 13.6 & 9.05 & 6.41 & 21.7 & 10.4 & 54.7 \\ % Final
      & \quad\quad\quad + copying \\
      \bottomrule
	\end{tabularx}
\end{table*}

\begin{table*}[t]
   \caption {Named entity, personal name, and rare proper noun recall \&
             precision on GoodNews and NYTimes800k.}
	\label{tab:results-names}
	\centering
	\begin{tabularx}{\textwidth}{llXXXXXX}
		\toprule
      &  & \multicolumn{2}{c}{Named entities} & \multicolumn{2}{c}{Personal names} & \multicolumn{2}{c}{Rare proper nouns} \\
      &  & Recall  & Precision & Recall  & Precision & Recall  & Precision \\
      \midrule
      \multirow{8}{*}{\rotatebox[origin=c]{90}{GoodNews}}
      & Biten (Avg + CtxIns) \cite{Biten2019GoodNews} & 6.06\% & 8.23\% & 6.55\% & 9.38\% & -- & -- \\
      & Biten (TBB + AttIns) \cite{Biten2019GoodNews} & 5.64\% & 8.87\% & 6.98\% & 11.9\% & -- & -- \\
      \cmidrule{2-8}
      & GloVe + LSTM & & &  &  & -- & --  \\
      & GloVe + Transformer & & &  &  & -- & -- \\
      & RoBERTa + LSTM & &  &  &  & -- & -- \\
      & RoBERTa + Transformer & 18.4\% & 21.6\% & 22.4\% & 28.1\% & -- & -- \\
      & \quad + faces & 18.7\% & 22.1\% & 23.2\% & 29.2\% & -- & -- \\
      & \quad\quad + copying \\
      \midrule
      \multirow{7}{*}{\rotatebox[origin=c]{90}{NYTimes800k}}
      & GloVe + LSTM & 7.26\% & 10.2\% & 5.69\% & 8.61\% & 0\% & 0\%  \\
      & GloVe + Transformer & 10.9\% & 13.4\% & 9.50\% & 13.5\% & 0\% & 0\%  \\
      & RoBERTa + LSTM & 15.0\% & 17.1\% & 18.1\% & 22.6\% & 15.1\% & 15.2\% \\
      & RoBERTa + Transformer & 19.5\% & 21.0\% & 25.5\% & 30.3\% & 22.6\% & 29.1\% \\
      & \quad + location-aware & 21.9\% & 24.1\% & 30.2\% & 35.5\% & 26.2\% & 32.5\% \\
      & \quad\quad + faces & 22.3\% & 24.5\% & 31.3\% & 37.1\% & 26.6\% & 33.6\% \\
      & \quad\quad\quad + copying \\
		\bottomrule
	\end{tabularx}
\end{table*}

\begin{table}[t]
   \caption {Linguistic measures on the generated captions: caption length (CL),
             type-token ratio (TTR), and Flesch readability ease (FRE).}
	\label{tab:results-stats}
	\centering
	\begin{tabularx}{\linewidth}{llXXX}
		\toprule
       &  &  CL  & TTR & FRE \\
      \midrule
      \multirow{9}{*}{\rotatebox[origin=c]{90}{GoodNews}}
      & Ground Truths & 18.1 & 94.9 & 65.4 \\
      \cmidrule{2-5}
      & Biten (Avg + CtxIns) \cite{Biten2019GoodNews}  & 9.9 & 92.2 & 78.3 \\
      % Biten (Wavg + CtxIns) \cite{Biten2019GoodNews}  & 9.2 &  \\
      % Biten (TBB + CtxIns) \cite{Biten2019GoodNews}  & 9.2 &  \\
      % Biten (Avg + AttIns) \cite{Biten2019GoodNews}  & 9.7 &  \\
      % Biten (Wavg + AttIns) \cite{Biten2019GoodNews}  & 9.0 &  \\
      & Biten (TBB + AttIns) \cite{Biten2019GoodNews}  & 9.1 & 90.7 & 77.6 \\
      \cmidrule{2-5}
      & GloVe + LSTM &  &  \\
      & GloVe + Transformer &  &  \\
      & RoBERTa + LSTM &  &   \\
      & RoBERTa + Transformer & 15.5 & 91.0 & 72.0 \\
      & \quad + faces & 15.5 & 90.7 & 71.9 \\
      & \quad\quad + copying \\
      \midrule
      \multirow{8}{*}{\rotatebox[origin=c]{90}{NYTimes800k}}
      & Ground Truths & 18.4 & 94.6 & 63.9 \\
      & GloVe + LSTM  & 13.8 & 89.0 & 77.8 \\
      & GloVe + Transformer  & 15.1 & 88.6 & 73.8 \\
      & RoBERTa + LSTM  & 14.9 & 90.2 & 72.6 \\
      & RoBERTa + Transformer  & 15.3 & 91.5 & 70.4 \\
      & \quad + location-aware & 15.1 & 91.7 & 70.4  \\
      & \quad\quad + faces & 15.2 & 91.6 & 70.5 \\
      & \quad\quad\quad + copying \\
		\bottomrule
	\end{tabularx}
\end{table}



\section{Model Architecture}

\subsection{Model Inputs}

Our proposed model takes three inputs: the image, the faces, and article text.
Each of these inputs first goes through an encoder to give us a vector
representation.

\subsubsection{Image Embedding}

For the image, we feed it through a pretrained ResNet-152 model
\cite{He2016ResNet} and use the output of the final block, just before the
pooling layer, as the image embedding $\bm{x}_I \in \mathbb{R}^{2048 \times 7
\times 7}$. The embedding forms a 7 by 7 block, allowing the transformer to
attend to 49 different patches in the image.

\subsubsection{Article Embedding}

For the article, we use RoBERTa \cite{Liu2019RoBERTaAR} to encode the text.
RoBERTa is a language representation model that provides pretrained contextual
embeddings for text. It consists of 24 layers of bidirectional transformer
blocks. It is a more carefully trained BERT \cite{Devlin2019BERT} model.

Unlike GloVe \cite{Pennington2014Glove} and word2vec
\cite{Mikolov2013DistributedRO} embeddings, where each word has exactly one
representation, the bidirectionality and the attention mechanism in the
transformer allow a word to have different vector representations depending on
the surrounding context.

The largest GloVe model has a vocabulary size of 1.2 million. Although this is
large, many rare names will still get mapped to the unknown token. In contrast,
RoBERTa uses BPE \cite{Sennrich2015NeuralMT,Radford2019LanguageMA} which can
encode any word that can be written in Unicode characters.

Given an input of length $S$, the pretrained RoBERTa encoder will return 25
sequences of embeddings, $\bm{h}_i \in \mathbb{R}^{2048 \times S}$ for $i \in
\{0,1, 2,...,24\}$. This includes the initial uncontextualized embeddings and
the output of each of the 24 layers. Inspired by Tenney \etal
\cite{Tenney2019BertRT}, who showed that different layers in BERT represent
different steps in the traditional NLP pipeline, we take a weighted sum
across all layers to obtain the article embedding:
\begin{IEEEeqnarray*}{lCl}
   \bm{x}_T &=& \sum_{i=0}^{24} w_i \bm{h}_i
\end{IEEEeqnarray*}
where $\bm{x}_T \in \mathbb{R}^{1024 \times S}$ is the article embedding
and $w_i$ are learnable weights.

One limitation of RoBERTa is that the maximum length of the input sequence is
512. For GoodNews, we simply encode the first 512 tokens of the article. For
NYTimes800k, since we have the image position, we concatenate the title, the
first paragraph, and as many paragraphs above and below the image as we can
fit, until we reach the 512 token limit. Note that since we are using BPE, a
token can be part of a word. On average, we can only encode ...... words of
the article.


\subsubsection{Face Embedding}

To embed the faces, we first use MTCNN \cite{Zhang2016JointFD} to detect the
face bounding boxes. We then select the top $M$ faces and feed them through a
FaceNet model \cite{Schroff2015FaceNetAU}, pretrained on the VGGFace2 dataset
\cite{Cao2017VGGFace2AD}, to obtain a face embedding $\bm{x}_F \in
\mathbb{R}^{512 \times M}$ where $M$ is the number of faces.

\subsection{Decoder}

\subsubsection{Dynamic Convolutions}

Our decoder is a transformer architecture with dynamic convolutions
\cite{Wu2018PayLA} containing four layers with kernel sizes 3, 7, 15 and 31. We
found that using dynamic convolutions allows the training to converge faster
than using the current state-of-the-art GPT-2 architecture
\cite{Radford2019LanguageMA}. We don't have the computational resource to study
the performance of GPT-2.

\subsubsection{Adaptive Softmax}

To make training more efficient, we use adaptive softmax
\cite{Grave2016EfficientSA} and divide the vocabulary into three clusters: 5K,
15K, and 25K. We tie the adaptive weights and we share the decoder input and
output embeddings. We use sinusoidal positional encoding
\cite{Vaswani2017AttentionIA} to represent the position of each token.

\subsubsection{Encoder Attention}

For each generation step, we use the Scalar-Dot-Product attention
\cite{Vaswani2017AttentionIA} to attend to the image and to article separate.
The two attended representations are then concatenated and fed as input to
the self-attention layer.


\section{Experiments}

\subsection{Training Details}

In all experiments, we use Adam \cite{Kingma2015Adam} with the weight decay fix
\cite{Loshchilov2018DecoupledWD} to optimize the models. We use the following
parameters: $\beta_1 = 0.9, \beta_2 = 0.98, \epsilon = 10^{-6}$, and a weight
decay of $10^{-5}$. We clip the gradient norm at 0.1. All models see the same
number of training examples, which is 6.6 million. This is equivalent to 16
epochs through GoodNews and 9 epochs through NYTimes800k. We warm up the
learning rate in the first 5\% of the training steps to $10^{-4}$, and
afterward decays it linearly. Training the full model takes 5 days on one Titan
V.

The training pipeline is written in PyTorch \cite{Paszke2017Automatic} using
the AllenNLP framework \cite{Gardner2017AllenNLP}. The RoBERTa model and
dynamic convolution code are adapted from fairseq \cite{Ott2019Fairseq}.

\subsection{Results}

Table \ref{tab:results} shows the results on GoodNews and NYTimes800k.


\begin{table}[t]
	\caption {Model complexity}
	\label{tab:models}
	\centering
	\begin{tabularx}{\linewidth}{Xc}
		\toprule
        & No. of Parameters \\
      \midrule
      GloVe + LSTM & 157M \\
      GloVe + Transformer & 148M \\
      RoBERTa + LSTM & 159M \\
      RoBERTa + Transformer & 154M \\
      \quad + faces & 171M \\
      \quad\quad + copying & 207M \\
		\bottomrule
	\end{tabularx}
\end{table}

\section{Discussion}




\section{Conclusion}

{\small
\bibliographystyle{ieee_fullname}
\bibliography{paper}
}

\end{document}
