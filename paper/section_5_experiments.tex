% !TEX root = main.tex

\section{Experiments}

\subsection{Training Details}

For parameter optimisation we use the adaptive gradient algorithm
Adam~\cite{Kingma2015Adam} with the following
parameter settings: $\beta_1 = 0.9, \beta_2 = 0.98, \epsilon = 10^{-6}$. We
warm up the
learning rate in the first 5\% of the training steps to $10^{-4}$, and decay
it linearly afterwards.

We apply $L_2$ regularisation to all network weights with a value of $10^{-5}$
and use the weight decay fix~\cite{Loshchilov2018DecoupledWD} to decouple the
learning rate from the regularisation parameter. We clip the gradient norm at
0.1. We \verify{ use a batch size of ****** and train all models for 6.6 million
steps}. This is equivalent to 16
epochs on GoodNews and 9 epochs on NYTimes800k. Training is done with mixed
precision to reduce the
memory footprint and allow our full model to be trained on a single GPU. This
full model
takes 5 days to train on one Titan V GPU and
has 207 million trainable parameters -- see the supplement for the number of
trainable parameters in each model variant.

The training pipeline is written in PyTorch~\cite{Paszke2017Automatic} using
the AllenNLP framework~\cite{Gardner2017AllenNLP}. The RoBERTa model and
dynamic convolution code are adapted from fairseq~\cite{Ott2019Fairseq}.

\subsection{Evaluation Metrics}

We use BLEU~\cite{Papineni2002Bleu}, ROUGE
\cite{Lin2004ROUGE}, METEOR~\cite{Denkowski2014Meteor}, and CIDEr
\cite{Vedantam2015CIDEr} scores as they are standard for evaluating image
captions. These are obtained using the COCO
caption evaluation toolkit
\footnote{\href{https://github.com/tylin/coco-caption}
{https://github.com/tylin/coco-caption}}. In addition, we evaluate the
precision and recall on: named entities, personal names, and rare proper names.
\verify{Named entities are identified in both the ground truth captions and
the generated
captions using the SpaCy~\cite{} natural language processing toolkit. We then
count exact string matches between the ground truth entities and generated
entities.
For personal names we restrict the set of named entities to those marked as
PERSON by the SpaCy parser. Rare proper nouns are nouns (as identified by the
SpaCy Parts-of-Speech tagger) that occur less than ******** times in the
training
dataset.}

\subsection{Baselines and Model Variants}

We show two previous state-of-the-art
models \textit{Biten (Avg + CtxIns)} and \textit{Biten (TBB +
	AttIns)}~\cite{Biten2019GoodNews}. To provide a fair comparison we used the
full caption results released by Biten \etal~\cite{Biten2019GoodNews} and
re-evaluated them using our evaluation pipeline and slightly different test set
(which we
have because of
broken image URLs). The final metrics are the same as originally reported if
rounded to the nearest
whole number.

We evaluate a few key modelling choices: the decoder type (\textit{LSTM} or
\textit{Transformer}), the text encoder type (\textit{GloVe} or
\textit{weighted RoBERTa}), the additional context domains
(\textit{location-aware} and \textit{face attention}), and word copying
(\textit{copying}). \verify{We keep the total number of trainable parameter for
each
model to within 7\% of one another other (148 million to 159 million), with the
unavoidable exceptions of
models employing \textit{location-aware} (*****), \textit{face attention} (171
million), and
\textit{copying} (207 million).}


\subsection{Results}


Table~\ref{tab:results} shows the BLEU, ROUGE, METEOR, and CIDEr metrics on
GoodNews and NYTimes800k.

\eat{The due primarily to broken image links the numbers in the table are
slightly different to what was previously reported~\cite{Biten2019GoodNews}.
Note that
the numbers reported here are slightly different from the original paper since
we had to remove a few samples from the test set where the image is no longer
available.~\cite{Biten2019GoodNews} also did some post-processing on the
ground-truth captions such as removing contractions and non-ASCII characters,
both of which we did not do. Despite these differences, the final metrics
are the same if rounded to the nearest whole number.}

There is a strong correlation between of all these metrics, and in general, we
mainly look at CIDEr since it uses Term Frequency Inverse Document Frequency
(TF-IDF) to put more importance on less common words such as entity names.
Table~\ref{tab:names} shows the recall and precision of the named entities,
personal names, and rare proper nouns.

We can make the following observations:

\begin{itemize}
   \item Our baseline LSTM model with GloVe embeddings yields competitive
   results to previous the state-of-the-art~\cite{Biten2019GoodNews}. This
   means that BPE offers a viable alternative to template-based methods.

   \item Models with GloVe embeddings are unable to generate rare proper nouns.
   This is expected since GloVe has a fixed vocabulary and if there is a
   unknown word in the article, the encoder will simply skip it.

   \item Switching from an LSTM to a transformer architecture improves the
   CIDEr score on NYTimes800k by 8 points, from 12 to 20. If we then use the
   contextualize RoBERTa embeddings instead of GloVe, CIDEr more than doubles
   to 44.

   \item Adding attention over the faces improves both the recall and precision
   of personal names. It has no significant effect on other entity types (see
   the supplementary materials for a detailed breakdown).
\end{itemize}


Table~\ref{tab:names} also looks at the quality of the generated captions. We
look at three metrics: caption length, type-token ratio (TTR), and Flesch
reading ease. TTR is the ratio of the number of unique words to the total
number of words in a caption. The Flesch reading ease takes into account the
number of words and syllables and produces a score between 0 and 100, where
higher means being easier to read.

From these metrics, we see that our generated captions are in general still
shorter than real-life captions, have lower lexical diversity (lower TTR)
and still use simpler language (higher Flesch reading ease).
