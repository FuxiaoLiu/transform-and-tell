% !TEX root = main.tex


\section{Experiments}

\subsection{Training Details}

In all experiments, we use Adam \cite{Kingma2015Adam} with the weight decay fix
\cite{Loshchilov2018DecoupledWD} to optimize the models. We use the following
parameters: $\beta_1 = 0.9, \beta_2 = 0.98, \epsilon = 10^{-6}$, and a weight
decay of $10^{-5}$. We clip the gradient norm at 0.1. All models see the same
number of training examples, which is 6.6 million. This is equivalent to 16
epochs through GoodNews and 9 epochs through NYTimes800k. We warm up the
learning rate in the first 5\% of the training steps to $10^{-4}$, and
afterward decays it linearly. Training the full model takes 5 days on one Titan
V GPU.

As shown in Table \ref{tab:models}, all of our baselines are designed to have
roughly the same number of trainable parameters. This allows us to attribute
improvement to the individual model components rather than to simply having a
bigger model.

The training pipeline is written in PyTorch \cite{Paszke2017Automatic} using
the AllenNLP framework \cite{Gardner2017AllenNLP}. The RoBERTa model and
dynamic convolution code are adapted from fairseq \cite{Ott2019Fairseq}.

\begin{table}[t]
	\caption {Model complexity}
	\label{tab:models}
	\centering
	\begin{tabularx}{\linewidth}{Xc}
		\toprule
        & No. of Parameters \\
      \midrule
      LSTM + GloVe & 157M \\
      Transformer + GloVe & 148M \\
      LSTM + weighted RoBERTa & 159M \\
      \midrule
      Transformer + RoBERTa & 154M \\
      \quad + weighted RoBERTa & 154M \\
      \quad\quad + face attention & 171M \\
      \quad\quad\quad + copy mechanism & 207M \\
		\bottomrule
	\end{tabularx}
\end{table}

\subsection{Results}

\begin{table}[t]
   \caption {Linguistic measures on the generated captions: caption length (CL),
             type-token ratio (TTR), and Flesch readability ease (FRE).}
	\label{tab:caption}
	\centering
	\begin{tabularx}{\linewidth}{llXXX}
		\toprule
       &  &  CL  & TTR & FRE \\
      \midrule
      \multirow{10}{*}{\rotatebox[origin=c]{90}{GoodNews}}
      & Ground Truths & 18.1 & 94.9 & 65.4 \\
      \cmidrule{2-5}
      & Biten (Avg + CtxIns) \cite{Biten2019GoodNews}  & 9.9 & 92.2 & 78.3 \\
      % Biten (Wavg + CtxIns) \cite{Biten2019GoodNews}  & 9.2 &  \\
      % Biten (TBB + CtxIns) \cite{Biten2019GoodNews}  & 9.2 &  \\
      % Biten (Avg + AttIns) \cite{Biten2019GoodNews}  & 9.7 &  \\
      % Biten (Wavg + AttIns) \cite{Biten2019GoodNews}  & 9.0 &  \\
      & Biten (TBB + AttIns) \cite{Biten2019GoodNews}  & 9.1 & 90.7 & 77.6 \\
      \cmidrule{2-5}
      & Transformer + RoBERTa \\
      & LSTM + GloVe & 13.8 & 89.6 & 77.5 \\
      & Transformer + GloVe & 15.5 & 88.5 & 73.9 \\
      & LSTM + weighted RoBERTa &  &   \\
      \cmidrule{2-5}
      & \quad + weighted RoBERTa & 15.5 & 91.0 & 72.0 \\
      & \quad\quad + face attention & 15.5 & 90.7 & 71.9 \\
      & \quad\quad\quad + copy mechanism \\
      \midrule
      \multirow{9}{*}{\rotatebox[origin=c]{90}{NYTimes800k}}
      & Ground Truths & 18.4 & 94.6 & 63.9 \\
      & LSTM + GloVe  & 13.8 & 89.0 & 77.8 \\
      & Transformer + GloVe  & 15.1 & 88.6 & 73.8 \\
      & LSTM + weighted RoBERTa  & 14.9 & 90.2 & 72.6 \\
      \cmidrule{2-5}
      & Transformer + RoBERTa \\
      & \quad + weighted RoBERTa  & 15.3 & 91.5 & 70.4 \\
      & \quad\quad + location-aware & 15.1 & 91.7 & 70.4  \\
      & \quad\quad\quad + face attention & 15.2 & 91.6 & 70.5 \\
      & \quad\quad\quad\quad + copy mechanism \\
		\bottomrule
	\end{tabularx}
\end{table}

\begin{figure*}[t]
   \begin{center}
   \fbox{\rule{0pt}{3in} \rule{.9\linewidth}{0pt}}
   \end{center}
      \caption{Example captions with success and failure cases.}
   \label{fig:short}
\end{figure*}


Table \ref{tab:results} shows the BLEU \cite{Papineni2002Bleu}, ROUGE
\cite{Lin2004ROUGE}, METEOR \cite{Denkowski2014Meteor}, and CIDEr
\cite{Vedantam2015CIDEr} metrics on GoodNews and NYTimes800k. We show two best
results from the previous state-of-the-art \cite{Biten2019GoodNews}. Note that
the numbers reported here are slightly different from the original paper since
we had to remove a few samples from the test set where the image is no longer
available. \cite{Biten2019GoodNews} also did some post-processing on the
ground-truth captions such as removing contractions and non-ASCII characters,
both of which we did not do. Despite these differences, the final metrics
are the same if rounded to the nearest whole number.

There is a strong correlation between of all these metrics, and in general, we
mainly look at CIDEr since it uses Term Frequency Inverse Document Frequency
(TF-IDF) to put more importance on less common words such as entity names.
Table \ref{tab:names} shows the recall and precision of the named entities,
personal names, and rare proper nouns.

We can make the following observations:

\begin{itemize}
   \item Our baseline LSTM model with GloVe embeddings yields competitive
   results to previous the state-of-the-art \cite{Biten2019GoodNews}. This
   means that BPE offers a viable alternative to template-based methods.

   \item Models with GloVe embeddings are unable to generate rare proper nouns.
   This is expected since GloVe has a fixed vocabulary and if there is a
   unknown word in the article, the encoder will simply skip it.

   \item Switching from an LSTM to a transformer architecture improves the
   CIDEr score on NYTimes800k by 8 points, from 12 to 20. If we then use the
   contextualize RoBERTa embeddings instead of GloVe, CIDEr more than doubles
   to 44.

   \item Adding attention over the faces improves both the recall and precision
   of personal names. It has no significant effect on other entity types (see
   the supplementary materials for a detailed breakdown).
\end{itemize}


Table \ref{tab:caption} look at the quality of the generated captions. We look
at three metrics: caption length, type-token ratio (TTR), and Flesch reading
ease. TTR is the ratio of the number of unique words to the total number of
words in a caption. The Flesch reading ease takes into account the number of
words and syllables and produces a score between 0 and 100, where higher means
being easier to read.

From these metrics, we see that our generated captions are in general still
shorter than real-life captions, have lower lexical diversity (lower TTR)
and still use simpler language (higher Flesch reading ease).
