% !TEX root = main.tex

\section{Introduction}

Most existing captioning systems can only generate a generic description of an
image \cite{Donahue2015LongTR, Vinyals2015ShowAT, Fang2015FromCT,
Karpathy2015DeepVA, Rennie2017SelfCriticalST, Lu2017KnowingWT,
Anderson2017BottomUpAT, Cornia2019ShowCT}, mainly because these architectures
cannot handle an open vocabulary and early captioning datasets such as MS COCO
\cite{Lin2014MicrosoftCC, Chen2015MicrosoftCC} and Flickr30k
\cite{Young2014FromID} were annotated using only visual cues present in the
image. More recently, datasets such as TIME \cite{Tariq2017ACE}, BreakingNews
\cite{Ramisa2016BreakingNewsAA}, and GoodNews \cite{Biten2019GoodNews} include
real-life captions written by professional journalists. However, existing
models that are trained on these news datasets still need to rely on either
extractive methods or templates to insert named entities.

This motivates us to ask whether it is possible to design a fully end-to-end
model for news image captioning. The model would need to 1) attend to
contextual cues from different modalities and 2) generate captions that contain
specific information including rare proper names that have not been seen in the
training data. To answer this, we look at the recent development in
sequence-to-sequence architectures and language representation learning.

The most successful family of models in the field of natural language
processing (NLP) are the transformer networks, which consist of layers of
self-attention and feed-forward connections stacked on top of each other
\cite{Vaswani2017AttentionIA}. Transformers have consistently been shown to
outperform RNN architectures in language modeling \cite{Radford2019LanguageMA},
story generation \cite{Fan2018HierarchicalNS}, summarization
\cite{Subramanian2019OnEA}, and machine translation \cite{Bojar2018Findings}.
There are theoretical justifications that transformers could be universal
approximations of sequence-to-sequence functions \cite{Anonymous2020AreTU}.

Furthermore, transformers have led to the rise of unsupervised language
representation learning such as BERT \cite{Devlin2019BERT}, XLM
\cite{Lample2019CrosslingualLM}, XLNet \cite{Yang2019XLNetGA}, RoBERTa
\cite{Liu2019RoBERTaAR}, and ALBERT \cite{Lan2019ALBERT}. The contextual word
embeddings provided by these methods have helped establish new state-of-the-art
results on many natural language understanding benchmarks including GLUE
\cite{Wang2019GLUE}, SuperGLUE \cite{Wang2019SuperGLUEAS}, and SQuAD
\cite{Rajpurkar2016SQuAD, Rajpurkar2018KnowWY}, even surpassing the human
baselines in many cases.

\begin{figure}[t]
   \begin{center}
      \includegraphics[width=0.99\linewidth, frame]{figures/figure_1.pdf}
   \end{center}
      \caption{Our transformer model attends to embeddings from three different
      domains (image patches, faces, and article text). Using byte-pair
      encoding, the model can then directly produce a caption containing
      specific named entities without the use of templates.}
   \label{fig:long}
   \label{fig:onecol}
\end{figure}

In parallel to the development of these pre-training methods, many novel
techniques have been proposed to improve training convergence and handle more
diverse data inputs. The most significant contribution is the use of byte-pair
encoding (BPE) \cite{Sennrich2015NeuralMT} to represent a rare word as a
sequence of subword units, thus giving models the ability to handle an open
vocabulary.

The goal of this paper is to make use of these recent developments, along with
their bags of tricks, to carefully design a captioning system most appropriate
for news images. Our main contributions are thus threefold:

\begin{enumerate}
   \item We introduce NYTimes800k, the largest news image captioning dataset to
   date, containing 446K articles and 794K images with captions from The New
   York Times spanning 14 years. NYTimes800k is built upon the GoodNews dataset,
   but we write a custom parser to collect higher-quality articles and metadata
   such as the location of an image within the page.

   \item We build a captioning model that combines the power of transformers,
   byte-pair encoding, and attention over three different modalities (text,
   images, and faces). We show that our model achieves state-of-the-art results
   with a significant margin over previous methods, and in particular, it can
   generate names not seen during training without the use of templates.

   \item We provide a detailed model analysis, deconstructing the most
   important modeling components and quantifying the incremental contribution
   that each of them makes not only to the usual metrics such as BLEU, ROUGE,
   METEOR, and CIDEr; but also to other linguistic measures like readability
   scores, caption length, and recall of rare names.
\end{enumerate}
