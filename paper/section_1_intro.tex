% !TEX root = main.tex

\section{Introduction}



The internet is home to a huge number of images, many of which lack useful
captions. A growing body of work seeks to automatically generate captions that
describe the objects and relationships using only visual cues extracted from
the image itself~\cite{Donahue2015LongTR, Vinyals2015ShowAT, Fang2015FromCT,
Karpathy2015DeepVA, Rennie2017SelfCriticalST, Lu2017KnowingWT,
Anderson2017BottomUpAT, Cornia2019ShowCT}. While generic image descriptions
have their uses, such as for individuals with vision impairments, they are
often of less benefit to the average user. To produce more useful image
captions we need to go beyond generic descriptions and introduce information
that cannot be gleaned directly from the image alone. Fortunately, many images
have an associated context such as a news article, web page, or social
media post, which give the image greater meaning than can be extracted from its
pixels. To generate captions that go beyond generic description and actually
add information that could not be gleaned from the image alone we must take
this context into account. We focus on the news image captioning task in order
to design practical methods for exploiting contextual information.

\eat{However, images on the internet are
often associated with a context such as a news article, web page, or social
media post, which give the image greater meaning than can be extracted from its
pixels. To generate captions that go beyond generic description and actually
add information that could not be gleaned from the image alone we must take
this context into account.}

News image captioning is an interesting instance of contextual captioning
where news articles provide context to images.

Captions for news images, such as the example in Figure~\ref{fig:teaser},
typically contain details which cannot be derived from the image alone. They
also frequently contain proper nouns such as names of people, places, and
organisations -- in many cases these proper nouns are rare (most people and
places do not have many news articles written about them). A system capable of
generating high quality news captions should therefor make extensive use of the
provided context and be tuned for generating rare proper nouns. Existing
approaches to news image captioning~\cite{Tariq2017ACE,
Ramisa2016BreakingNewsAA,
	Biten2019GoodNews}  rely on text extraction or
template filling to deal with rare contextual terms such as names of people and
organisations. This makes them relatively inflexibility and means they cannot
be trained end-to-end. Moreover, existing approaches do not include specialised
visual models for frequent nouns -- experiments on the MSCOCO dataset have
shown
that pre-trained object detectors tuned for frequent nouns
lead to more accurate captions~\cite{}.

One tool that has had seen recent successes in many natural language processing
tasks is the transformer neural network.
%\sout{processing, which consist of layers of
%	self-attention and feed-forward connections stacked on top of each other
%	\cite{Vaswani2017AttentionIA}. Transformers have}
Transformers have been show to consistently
outperforming Recurrent Neural Network architectures in language modeling
\cite{Radford2019LanguageMA},
story generation \cite{Fan2018HierarchicalNS}, summarization
\cite{Subramanian2019OnEA}, and machine translation \cite{Bojar2018Findings}.
%\sout{There are theoretical justifications that transformers could be universal
%	approximations of sequence-to-sequence functions
%\cite{Anonymous2020AreTU}.}
Furthermore, transformer based models such as BERT \cite{Devlin2019BERT}, XLM
\cite{Lample2019CrosslingualLM}, XLNet \cite{Yang2019XLNetGA}, RoBERTa
\cite{Liu2019RoBERTaAR}, and ALBERT \cite{Lan2019ALBERT} have been shown to
produce high level text representations suitable for transfer learning.
Furthermore, using byte-pair
encoding (BPE) \cite{Sennrich2015NeuralMT} to represent uncommon words as a
sequence of subword units can enable the transformer function in an open
vocabulary setting.

\eat{The contextual word
	embeddings provided by these methods have helped establish new
	state-of-the-art
	results on many natural language understanding benchmarks including GLUE
	\cite{Wang2019GLUE}, SuperGLUE \cite{Wang2019SuperGLUEAS}, and SQuAD
	\cite{Rajpurkar2016SQuAD, Rajpurkar2018KnowWY}, even surpassing the human
	baselines in many cases.}

\eat{In parallel to the development of these pre-training methods, many novel
techniques have been proposed to improve training convergence and handle more
diverse data inputs. The most significant contribution is the use of byte-pair
encoding (BPE) \cite{Sennrich2015NeuralMT} to represent a rare word as a
sequence of subword units, thus giving models the ability to handle an open
vocabulary.}


\begin{figure}[t]
	\begin{center}
		\includegraphics[width=0.99\linewidth, frame]{figures/figure_1.pdf}
	\end{center}
	\caption{Our transformer model attends to embeddings from three different
		domains (image patches, faces, and article text). Using byte-pair
		encoding, the model can then directly produce a caption containing
		specific named entities without the use of templates.}
	\label{fig:long}
	\label{fig:teaser}
\end{figure}

This motivates our novel fully end-to-end
model for news image captioning that 1) combines specialised modules for
incorporating and selectively attending to image features, human faces, and
news article text and
2) applies a state-of-the-art sequence generation model which is able to
generate rare tokens, such as proper names, even when they do not form part of
the
training data. Our model relies on a novel combination of sequence-to-sequence
architectures, language representation learning, and
vision systems.

%The key components of our approach are selectively attending to different
%aspects of the two modalities: image
%regions, faces, and text sequences.
%Using powerful pre-trained contextual language models to represent news
%articles -- allowing articles with text not see during training  tuples
%required for training.
%Using sub-word units, which when combined with pre-training, allow for the
%generation of tokens not seen during training.


%\sout{
%Most existing captioning systems can only generate a generic description
%of an
%image \cite{Donahue2015LongTR, Vinyals2015ShowAT, Fang2015FromCT,
%Karpathy2015DeepVA, Rennie2017SelfCriticalST, Lu2017KnowingWT,
%Anderson2017BottomUpAT, Cornia2019ShowCT}, mainly because these architectures
%cannot handle an open vocabulary and early captioning datasets such as MS COCO
%\cite{Lin2014MicrosoftCC, Chen2015MicrosoftCC} and Flickr30k
%\cite{Young2014FromID} were annotated using only visual cues present in the
%image. More recently, datasets such as TIME \cite{Tariq2017ACE}, BreakingNews
%\cite{Ramisa2016BreakingNewsAA}, and GoodNews \cite{Biten2019GoodNews} include
%real-life captions written by professional journalists. However, existing
%models that are trained on these news datasets still need to rely on either
%extractive methods or templates to insert named entities.}




In this paper we carefully consider the news image captioning problem and
select a set of modelling tools which we combine into a novel architecture that
sets a new state-of-the-art result. Our main contributions are threefold:

\eat{The goal of this paper is to make use of these recent developments, along
with
their bags of tricks, to carefully design a captioning system most appropriate
for news images.}

\begin{enumerate}
   \item We introduce NYTimes800k, the largest news image captioning dataset to
   date, containing 446K articles and 794K images with captions from The New
   York Times spanning 14 years. NYTimes800k builds on the GoodNews dataset;
   but we write a custom parser to collect higher-quality articles and metadata
   such as the location of an image within the page.

   \item We build a captioning model that combines the power of transformers,
   byte-pair encoding, and attention over three different modalities (text,
   images, and faces). We show that our model achieves state-of-the-art results
   with a significant margin over previous methods, and in particular, it can
   generate names not seen during training without the use of templates.

   \item We provide a detailed model analysis, deconstructing the most
   important modeling components and quantifying the incremental contribution
   that each of them makes not only to the usual metrics such as BLEU, ROUGE,
   METEOR, and CIDEr; but also to other linguistic measures like readability
   scores, caption length, and recall of rare names.
\end{enumerate}
